{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1Tx4Gs0XAbV6",
        "CFx3VdqPNh9d",
        "iquovfLLd-tH",
        "7DfwZa-ygokP"
      ],
      "mount_file_id": "1QJfFz7Aj86zDZsXOVQqodupeap6Mw6fM",
      "authorship_tag": "ABX9TyNAunbd7lKN2DpfFEhAxA0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/Assignment3/ML_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tx4Gs0XAbV6"
      },
      "source": [
        "#Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtBFGh_qAgIg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pickle\n",
        "import random\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFx3VdqPNh9d"
      },
      "source": [
        "# Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBDQnce8Njuq"
      },
      "source": [
        "class MyPreProcessor():\n",
        "  \"\"\"\n",
        "  My steps for pre-processing for the All datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def pre_process(self, dataset):\n",
        "    \"\"\"\n",
        "    Reading the file and preprocessing the input and output.\n",
        "    Note that you will encode any string value and/or remove empty entries in this function only.\n",
        "    Further any pre processing steps have to be performed in this function too. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    dataset : integer with acceptable values 0, 1, or 2\n",
        "    0 ->  Dataset\n",
        "    1 ->  Dataset\n",
        "    2 ->  Dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    if dataset == 0:\n",
        "      # df=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\",header=None)\n",
        "      \n",
        "      df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/mnist_train.csv.zip\")\n",
        "      X=df.iloc[:,1:]\n",
        "      y=df.iloc[:,0]\n",
        "      b = np.zeros((y.size, y.max()+1))\n",
        "      b[np.arange(y.size),y] = 1\n",
        "      y=b\n",
        "           \n",
        "    \n",
        "    elif dataset == 1:\n",
        "      df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeTrain.csv\",header=None)\n",
        "      X=df.iloc[:,1:].to_numpy()\n",
        "      y=df[0].to_numpy()\n",
        "\n",
        "    elif dataset == 2:\n",
        "      df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeValidation.csv\",header=None)\n",
        "      X=df.iloc[:,1:].to_numpy()\n",
        "      y=df[0].to_numpy()\n",
        "    \n",
        "    elif dataset == 3:\n",
        "      df= pickle.load(open(\"/content/drive/MyDrive/ML_Assignment3/train_CIFAR.pickle\",\"rb\"))\n",
        "      X=df['X']\n",
        "      y=df['Y']\n",
        "    elif dataset == 4:\n",
        "      df= pickle.load(open(\"/content/drive/MyDrive/ML_Assignment3/test_CIFAR.pickle\",\"rb\"))\n",
        "      X=df['X']\n",
        "      y=df['Y']\n",
        "    return X, y\n",
        "\n",
        "preprocessor = MyPreProcessor()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFXETQq-Ago6"
      },
      "source": [
        "#My Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTMl6_XAFan"
      },
      "source": [
        "class MyNeuralNetwork():\n",
        "    \"\"\"\n",
        "    My implementation of a Neural Network Classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh']\n",
        "    weight_inits = ['zero', 'random', 'normal']\n",
        "\n",
        "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
        "        \"\"\"\n",
        "        Initializing a new MyNeuralNetwork object\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers : int value specifying the number of layers\n",
        "\n",
        "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
        "\n",
        "        activation : string specifying the activation function to be used\n",
        "                     possible inputs: relu, sigmoid, linear, tanh\n",
        "\n",
        "        learning_rate : float value specifying the learning rate to be used\n",
        "\n",
        "        weight_init : string specifying the weight initialization function to be used\n",
        "                      possible inputs: zero, random, normal\n",
        "\n",
        "        batch_size : int value specifying the batch size to be used\n",
        "\n",
        "        num_epochs : int value specifying the number of epochs to be used\n",
        "        \"\"\"\n",
        "\n",
        "        if activation not in self.acti_fns:\n",
        "            raise Exception('Incorrect Activation Function')\n",
        "\n",
        "        if weight_init not in self.weight_inits:\n",
        "            raise Exception('Incorrect Weight Initialization Function')\n",
        "        \n",
        "        # np.random.seed(10)\n",
        "        self.n_layers=n_layers\n",
        "        self.layer_sizes=layer_sizes \n",
        "        self.activation=activation \n",
        "        self.learning_rate=learning_rate \n",
        "        self.weight_init=weight_init\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        weights={}\n",
        "        bias={}\n",
        "        for i in range(self.n_layers-1):\n",
        "          weights[i]=np.array(self.weight_func((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
        "          bias[i]=np.zeros(self.layer_sizes[i+1])\n",
        "\n",
        "        self.weights=weights\n",
        "        self.bias=bias\n",
        "        \n",
        "\n",
        "    def activation_func(self,X):\n",
        "      \"\"\"\n",
        "      Calculating the activation for a particular layer\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : 1-dimentional numpy array \n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "      x_derv : 1-dimensional numpy array after calculating the specified derivat function over X\n",
        "      \"\"\"\n",
        "      if self.activation==\"relu\":\n",
        "        return self.relu(X),self.relu_grad(X)\n",
        "      elif self.activation==\"sigmoid\":\n",
        "        return self.sigmoid(X),self.sigmoid_grad(X)\n",
        "      elif self.activation==\"linear\":\n",
        "        return self.linear(X),self.linear_grad(X)\n",
        "      elif self.activation==\"tanh\":\n",
        "        return self.tanh(X),self.tanh_grad(X)\n",
        "      else:\n",
        "        return self.softmax(X),self.softmax_grad(X)\n",
        "\n",
        "    def relu(self, X):\n",
        "      \"\"\"\n",
        "      Calculating the ReLU activation for a particular layer\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : 1-dimentional numpy array \n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "      \"\"\"\n",
        "      return X * (X>=0)\n",
        "\n",
        "    def relu_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        return 1*(X>=0)\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc= 1/(1+np.exp(-X))\n",
        "\n",
        "        return x_calc\n",
        "\n",
        "    def sigmoid_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        sig=self.sigmoid(X)\n",
        "        x_calc=sig*(1-sig)\n",
        "        return x_calc\n",
        "\n",
        "    def linear(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=X\n",
        "        return x_calc\n",
        "\n",
        "    def linear_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=np.ones(X.shape)\n",
        "        return x_calc\n",
        "\n",
        "    def tanh(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=np.tanh(X)\n",
        "        return x_calc\n",
        "\n",
        "    def tanh_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        tanh=self.tanh(X)\n",
        "        x_calc=1-tanh**2\n",
        "        return x_calc\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        expo = np.exp(X)\n",
        "        x_calc=expo/expo.sum(axis=1, keepdims = True)\n",
        "        return x_calc\n",
        "    \n",
        "\n",
        "    def weight_func(self,shape):\n",
        "      if self.weight_init==\"zero\":\n",
        "        return self.zero_init(shape)\n",
        "      elif self.weight_init==\"random\":\n",
        "        return self.random_init(shape)\n",
        "      else:\n",
        "        return self.normal_init(shape)\n",
        "\n",
        "    def zero_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Zero Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight= np.zeros(shape)\n",
        "        return weight \n",
        "\n",
        "    def random_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Random Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight= np.random.rand(shape[0],shape[1])*0.01\n",
        "        return weight \n",
        "\n",
        "    def normal_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight=np.random.normal(size=shape)\n",
        "        return weight\n",
        "    \n",
        "    def cross_entropy(self,y_hat,y):\n",
        "        samples=y.shape[0]\n",
        "        error=y_hat-y\n",
        "        return error\n",
        "\n",
        "    def cross_entropy_loss(self, A, y):\n",
        "        n = len(y)\n",
        "        logp = - np.log(A[np.arange(n), y.argmax(axis=1)])\n",
        "        loss = np.sum(logp)/n\n",
        "        return loss\n",
        "    \n",
        "\n",
        "    def fit(self, X, y,X_test=None,y_test=None):\n",
        "        \"\"\"\n",
        "        Fitting (training) the linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        self : an instance of self\n",
        "        \"\"\"\n",
        "        train_error=[]\n",
        "        test_error=[]\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "          for batch in range(0,X.shape[0],self.batch_size):\n",
        "            X_sample=deepcopy(X[batch:batch+self.batch_size,:])\n",
        "            y_sample=deepcopy(y[batch:batch+self.batch_size,:])\n",
        "            input=deepcopy(X_sample)\n",
        "            output=deepcopy(y_sample)\n",
        "\n",
        "            activations,preactivations = self.feed_forward(input)\n",
        "\n",
        "            dervs = self.backward_prop(output,activations,preactivations)\n",
        "\n",
        "            # Gradient updation\n",
        "            activations[-1]=X_sample\n",
        "            for layer in range(self.n_layers-1):\n",
        "              grad=activations[layer-1].T.dot(dervs[layer])/len(X_sample)\n",
        "              self.weights[layer]=self.weights[layer]-self.learning_rate*grad\n",
        "              self.bias[layer]=self.bias[layer]-self.learning_rate*np.sum(dervs[layer],axis=0)/len(X_sample)\n",
        "\n",
        "          if((epoch+1)%5==0):\n",
        "            train_cost = self.cross_entropy_loss(activations[self.n_layers-2],y_sample)\n",
        "            print(\"epoch\",epoch,\"\\t\",train_cost)\n",
        "          if(X_test is not None):\n",
        "            y_test_pred=self.predict_proba(X_test)\n",
        "            test_cost=self.cross_entropy_loss(y_test_pred,y_test)\n",
        "            train_cost = self.cross_entropy_loss(activations[self.n_layers-2],y_sample)\n",
        "            train_error.append(train_cost)\n",
        "            test_error.append(test_cost)\n",
        "            self.activations=activations\n",
        "            self.preactivations=preactivations\n",
        "\n",
        "        self.train_error=np.array(train_error)\n",
        "        self.test_error=np.array(test_error)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def feed_forward(self,input):\n",
        "      \"\"\"\n",
        "      Fitting (training) the linear model.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      input : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "      \n",
        "      Returns\n",
        "      -------\n",
        "      activations : dictionary of value of each layer after activation\n",
        "      preactivations : dictionary of value of each layer before activation\n",
        "      \"\"\"\n",
        "      preactivations={}\n",
        "      activations={}\n",
        "      for layer in range(self.n_layers-2):\n",
        "        hidden_output=input.dot(self.weights[layer])+self.bias[layer]\n",
        "        hidden_output_A,_=self.activation_func(hidden_output)          \n",
        "        input=hidden_output_A \n",
        "        preactivations[layer]=hidden_output  \n",
        "        activations[layer]=hidden_output_A\n",
        "\n",
        "      hidden_output=input.dot(self.weights[self.n_layers-2])+self.bias[self.n_layers-2]      \n",
        "      preactivations[self.n_layers-2]=hidden_output \n",
        "      activations[self.n_layers-2]=self.softmax(hidden_output)\n",
        "      return activations,preactivations\n",
        "\n",
        "    def backward_prop(self,y,activations,preactivations):\n",
        "      \"\"\"\n",
        "      Fitting (training) the linear model.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      activations : dictionary of value of each layer after activation\n",
        "\n",
        "      preactivations : dictionary of value of each layer before activation\n",
        "      \n",
        "      Returns\n",
        "      -------\n",
        "      dervs: gradients that will be used to update weights and biases\n",
        "      \"\"\"\n",
        "      dervs={}\n",
        "      y_pred=activations[self.n_layers-2]\n",
        "      delta=y_pred-y\n",
        "      dervs[self.n_layers-2]=delta\n",
        "      for layer in range(self.n_layers-3,-1,-1):\n",
        "        error=delta.dot(self.weights[layer+1].T)\n",
        "        _,derv=self.activation_func(preactivations[layer])\n",
        "        delta=error*derv\n",
        "\n",
        "        dervs[layer]=delta\n",
        "\n",
        "      return dervs\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicting probabilities using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the \n",
        "            class wise prediction probabilities.\n",
        "        \"\"\"\n",
        "        y,_=self.feed_forward(X)\n",
        "\n",
        "        # return the numpy array y which contains the probability of predicted values\n",
        "        return y[self.n_layers-2]\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "        \"\"\"\n",
        "        y=self.predict_proba(X)\n",
        "\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        return y.argmax(axis=1)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        acc : float value specifying the accuracy of the model on the provided testing set\n",
        "        \"\"\"\n",
        "\n",
        "        y_pred=self.predict(X)\n",
        "        y=y.argmax(axis=1)\n",
        "        acc=metrics.accuracy_score(y,y_pred)\n",
        "        return acc\n",
        "    \n",
        "    def asses(self):\n",
        "      plt.plot(range(self.num_epochs),self.train_error,label=\"Training error\")\n",
        "      plt.plot(range(self.num_epochs),self.test_error,label=\"Testing error\")\n",
        "      plt.legend()\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Error\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEVsh_lHOIiv"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3GmRAYX5HzM"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X, y = preprocessor.pre_process(0)\n",
        "print(X.shape,y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.10)\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3RRakSxOs59"
      },
      "source": [
        "classifier=MyNeuralNetwork(5,[784,256, 128, 64,10],'relu',0.1,'random',3000,100)\n",
        "classifier.fit(X_train,y_train,X_test,y_test)\n",
        "classifier.asses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0IUbYnDT3mw"
      },
      "source": [
        "act,pre=classifier.feed_forward(X_train)\n",
        "a=act[2]\n",
        "a.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08vsDh0YS8yl"
      },
      "source": [
        "tsne = TSNE(n_components=2, verbose=2, n_iter=1000)\n",
        "tsne_results = tsne.fit_transform(a)\n",
        "\n",
        "plt.figure(figsize=(16,10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXfNBuRmXuEG"
      },
      "source": [
        "sns.scatterplot(\n",
        "  x=tsne_results[:,0], y=tsne_results[:,1],\n",
        "  hue=y_train.argmax(axis=1),\n",
        "  palette=sns.color_palette(\"hls\", 10),\n",
        "  legend=\"full\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IVb2YY_XgXm"
      },
      "source": [
        "pickle.dump(classifier,open(\"linear_random.pkl\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lijo6vgo9_UD"
      },
      "source": [
        "## sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN2sKcox-Am-"
      },
      "source": [
        "# clf = MLPClassifier(activation=\"identity\", hidden_layer_sizes=(256, 128, 64),learning_rate_init=0.1,batch_size=3000, max_iter=100)\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=0.1, hidden_layer_sizes=(256,128,64), random_state=1, activation = 'logistic')\n",
        "clf.fit(X_train,y_train.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJNUNGUk_1nu"
      },
      "source": [
        "clf.score(X_test,y_test.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_bsd1yBQYn9"
      },
      "source": [
        "# Q3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWPNACnui3cx"
      },
      "source": [
        "X_train,y_train= preprocessor.pre_process(1)\n",
        "X_val,y_val= preprocessor.pre_process(2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X_train.shape,y_train.shape,X_val.shape,y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqAbf-7va8SN"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "    super(MLP,self).__init__()\n",
        "    self.input_fc = nn.Linear(input_dim, hidden_dim)\n",
        "    self.output_fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    h_1 = F.relu(self.input_fc(x))\n",
        "    y_pred = self.output_fc(h_1)\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwrWvudIzsAI"
      },
      "source": [
        "class MyDataset(data.Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    return self.X[i],self.y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei1QAlBBu48e"
      },
      "source": [
        "def train(model, train_iterator,val_iterator, optimizer, criterion, device,epochs,flag=True):  \n",
        "\n",
        "    ce_loss=[]\n",
        "    val_loss=[]\n",
        "    for epoch in range(epochs):\n",
        "      epoch_loss = 0\n",
        "      epoch_loss_val=0\n",
        "      for train,valid in zip(train_iterator,val_iterator):\n",
        "          # training\n",
        "          x,y = train\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          y_pred = model(x.float())\n",
        "          loss = criterion(y_pred, y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "          #validation\n",
        "          x_val,y_val = valid\n",
        "          x_val = x_val.to(device)\n",
        "          y_val = y_val.to(device)          \n",
        "          y_val_pred= model(x_val.float())\n",
        "          loss= criterion(y_val_pred, y_val)\n",
        "          epoch_loss_val += loss.item()\n",
        "      \n",
        "      ce_loss.append(epoch_loss/ len(train_iterator))\n",
        "      val_loss.append(epoch_loss_val/ len(val_iterator))\n",
        "\n",
        "      if((epoch+1)%100==0):\n",
        "        print(\"epoch:\",epoch+1,\"\\t\",epoch_loss/ len(train_iterator),epoch_loss_val/ len(val_iterator) )\n",
        "    if(not flag):\n",
        "      return ce_loss,val_loss\n",
        "    return np.mean(ce_loss),np.mean(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ALiJVz-iEr"
      },
      "source": [
        "## 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSmxd7YJEUla"
      },
      "source": [
        "### a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fAm1UOWylxS"
      },
      "source": [
        "hidden_units=[5, 20, 50, 100 ,200]\n",
        "input_size=128\n",
        "output_size=10\n",
        "\n",
        "ce_loss=[]\n",
        "val_loss=[]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "train_data=MyDataset(X_train,y_train)\n",
        "val_data=MyDataset(X_val,y_val)\n",
        "\n",
        "for h_unit in hidden_units:\n",
        "  print(h_unit)\n",
        "  train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = 1024)\n",
        "  val_iterator = data.DataLoader(val_data, batch_size = 126)\n",
        "  model = nn.Sequential(nn.Linear(input_size, h_unit),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(h_unit, output_size),\n",
        "                        nn.Softmax(dim=1))\n",
        "  # model=MLP(input_size,output_size,h_unit)\n",
        "  optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
        "\n",
        "  t_loss,v_loss=train(model,\n",
        "                      train_iterator,\n",
        "                      val_iterator,\n",
        "                      optimizer,\n",
        "                      criterion,\n",
        "                      device,\n",
        "                      500)\n",
        "  ce_loss.append(t_loss)\n",
        "  val_loss.append(v_loss)\n",
        "\n",
        "plt.plot(hidden_units,ce_loss,label=\"Average Training Loss\")\n",
        "plt.plot(hidden_units,val_loss,label=\"Average Validation Loss\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.xlabel(\"Number of Hidden Units\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR6Y8sqODueR"
      },
      "source": [
        "## 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vyNq-zLd4d2"
      },
      "source": [
        "### a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEje1AvmB-vz"
      },
      "source": [
        "learning_rates=[0.1, 0.01, 0.001]\n",
        "input_size=128\n",
        "hidden_size=4\n",
        "output_size=10\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_data=MyDataset(X_train,y_train)\n",
        "val_data=MyDataset(X_val,y_val)\n",
        "\n",
        "train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = 1024)\n",
        "val_iterator = data.DataLoader(val_data, shuffle = True, batch_size = 126)\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(hidden_size, output_size),\n",
        "                        nn.Softmax(dim=1))\n",
        "for lr in learning_rates:\n",
        "  print(lr)\n",
        "  optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "  t_loss,v_loss=train(model,\n",
        "                      train_iterator,\n",
        "                      val_iterator,\n",
        "                      optimizer,\n",
        "                      criterion,\n",
        "                      device,\n",
        "                      100,\n",
        "                      False)\n",
        "  \n",
        "  plt.figure()\n",
        "  plt.title(\"Learning Rate: \"+str(lr))\n",
        "  plt.plot(range(100),t_loss,label=\"Average Training Loss\")\n",
        "  # plt.plot(range(100),v_loss,label=\"Average Validation Loss\")\n",
        "  plt.ylabel(\"Cross Entropy Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iquovfLLd-tH"
      },
      "source": [
        "# Q4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHFQIYp0bl2e"
      },
      "source": [
        "X_train,y_train=preprocessor.pre_process(3)\n",
        "X_test,y_test=preprocessor.pre_process(4)\n",
        "\n",
        "X_train.shape,X_test.shape,y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DfwZa-ygokP"
      },
      "source": [
        "## 1) EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IdxVvhHgVie"
      },
      "source": [
        "df=pd.DataFrame(X_train)\n",
        "class_names=[\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWu7wXuhe0yM"
      },
      "source": [
        "for i in range(16)\n",
        "  plt.subplot(2,8,i+1)\n",
        "  data = X_train[i, :] \n",
        "  data = np.reshape(data, (32,32,3), order='F' ) \n",
        "  plt.imshow(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Bs1addzkW1p"
      },
      "source": [
        "def display_color_hists(images, labels, indices, class_names=class_names):\n",
        "    fig = plt.figure(figsize=(10,6))\n",
        "    n = 0\n",
        "    for i in indices:\n",
        "        plt.subplot(2,3,n+1)\n",
        "        plt.hist(images[i][:1024],color = \"red\")\n",
        "        plt.title(class_names[labels[i]])\n",
        "        n += 1\n",
        "        \n",
        "        plt.subplot(2,3,n+1)\n",
        "        plt.hist(images[i][1024:2048],color = \"green\")\n",
        "        plt.title(class_names[labels[i]])\n",
        "        n += 1\n",
        "        \n",
        "        plt.subplot(2,3,n+1)\n",
        "        plt.hist(images[i][2048:],color = \"skyblue\")\n",
        "        plt.title(class_names[labels[i]])\n",
        "        n += 1\n",
        "    plt.show()\n",
        "display_color_hists(X_train, y_train,[0,2] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7MmxUjTnaeG"
      },
      "source": [
        "np.unique(y_train, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woiuil05dUag"
      },
      "source": [
        "## 2) AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MBJ2BxRzpvD"
      },
      "source": [
        "class MyDataset1(data.Dataset): \n",
        "    def __init__(self, data, label, transform=None):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.transform = transform\n",
        "        self.img_shape = data.shape\n",
        "        \n",
        "    def __getitem__(self, index): \n",
        "        img_reshaped = np.transpose(np.reshape(self.data[index],(3, 32,32)))\n",
        "  \n",
        "        img = Image.fromarray(img_reshaped)\n",
        "        label = self.label[index]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img_to_tensor = transforms.ToTensor()\n",
        "            img = img_to_tensor(img)\n",
        "        return img, label\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYnLOJD1syrd"
      },
      "source": [
        "alexnet = models.alexnet(pretrained=True)\n",
        "alexnet.eval()\n",
        "alexnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxvepnO7xnbT"
      },
      "source": [
        "train_transform_aug = transforms.Compose([\n",
        "    transforms.Resize((40, 40)),       \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Pad(16),\n",
        "    transforms.Normalize( mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])]\n",
        "  )\n",
        "train_data=MyDataset1(X_train,y_train,train_transform_aug)\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_data,\n",
        "                          batch_size=X_train.shape[0], \n",
        "                          shuffle=True)\n",
        "\n",
        "test_data=MyDataset1(X_test,y_test,train_transform_aug)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_data,\n",
        "                          batch_size=X_test.shape[0], \n",
        "                          shuffle=True)\n",
        "\n",
        "len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bthVaZAjs399"
      },
      "source": [
        "for x,y in train_loader:\n",
        "  output=alexnet(x)\n",
        "  print(output.size())\n",
        "\n",
        "for x,y in test_loader:\n",
        "  output_test=alexnet(x)\n",
        "  print(output_test.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6fMbuUa2sR-"
      },
      "source": [
        "X_new=output.detach().numpy()\n",
        "X_test_new=output_test.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAuYgCgWxpgn"
      },
      "source": [
        "## 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKi1sy8bwuJj"
      },
      "source": [
        "class MyDataset2(data.Dataset):\n",
        "\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    return self.X[i],self.y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PorM2YJcy-u7"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, device,epochs):  \n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      epoch_loss = 0\n",
        "      for x,y in iterator:\n",
        "         \n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          y_pred = model(x.float())\n",
        "\n",
        "          model.zero_grad()\n",
        "\n",
        "          loss = criterion(y_pred, y)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "      print(epoch_loss/len(iterator))\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KPZdOzczjPL"
      },
      "source": [
        "model = nn.Sequential(\n",
        "          torch.nn.Linear(1000, 512),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(512, 256),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(256, 2),\n",
        "        ).to(device)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "train_data=MyDataset2(X_new,y_train)\n",
        "train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = 1024)\n",
        "\n",
        "train(model,train_iterator,optimizer,criterion,device,50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_CLZNBH4SOI"
      },
      "source": [
        "## 4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZLuSEbV0jWq"
      },
      "source": [
        "test_data=MyDataset2(X_test_new,y_test)\n",
        "test_iterator = data.DataLoader(test_data, shuffle = True, batch_size = 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igLHtwb5BQDj"
      },
      "source": [
        "model.eval()\n",
        "for x,y in test_iterator:\n",
        "  output=model(x)\n",
        "  print(output.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1OifQuFB7PV"
      },
      "source": [
        "ypred=output.detach().numpy()\n",
        "ypred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA3ckf0XFYvb"
      },
      "source": [
        "ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P91ALnHVFoRi"
      },
      "source": [
        "np.unique(y_test,return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIP1HJzJzNsy"
      },
      "source": [
        "### sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxnErtVJzPzw"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(512,256), max_iter=200, solver='sgd')\n",
        "mlp.fit(X_new, y_train)\n",
        "mlp.score(X_test_new,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12cz3BLP1xuO"
      },
      "source": [
        "y_pred=mlp.predict(X_test_new)\n",
        "metrics.confusion_matrix(y,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBAvU-zeeSXn"
      },
      "source": [
        "scores = mlp.predict_proba(X_test_new)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, scores[:,0], pos_label=2)\n",
        "scores[:,0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3NA_798e4su"
      },
      "source": [
        "metrics.plot_roc_curve(mlp, X_test_new, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQNyAKTcfSXE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nplsbN_6tFl"
      },
      "source": [
        "# Extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVTiR3hp6vyw"
      },
      "source": [
        "model=pickle.load(open(\"/content/sigmoid_normal.pkl\",\"rb\"))\n",
        "model.asses()\n",
        "model.score(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHmylgPj67s7"
      },
      "source": [
        "/content/linear_random.pkl\n",
        "/content/relu_random.pkl\n",
        "/content/sigmoid_normal.pkl\n",
        "/content/sigmoid_random.pkl\n",
        "/content/tanh_normal.pkl\n",
        "/content/tanh_random.pkl"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}