{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QJfFz7Aj86zDZsXOVQqodupeap6Mw6fM",
      "authorship_tag": "ABX9TyOe/39//NgJ9l4qdf84QWSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/Assignment3/ML_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tx4Gs0XAbV6"
      },
      "source": [
        "#Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtBFGh_qAgIg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFx3VdqPNh9d"
      },
      "source": [
        "# Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBDQnce8Njuq"
      },
      "source": [
        "class MyPreProcessor():\n",
        "  \"\"\"\n",
        "  My steps for pre-processing for the All datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def pre_process(self, dataset):\n",
        "    \"\"\"\n",
        "    Reading the file and preprocessing the input and output.\n",
        "    Note that you will encode any string value and/or remove empty entries in this function only.\n",
        "    Further any pre processing steps have to be performed in this function too. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    dataset : integer with acceptable values 0, 1, or 2\n",
        "    0 ->  Dataset\n",
        "    1 ->  Dataset\n",
        "    2 ->  Dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "    \"\"\"\n",
        "    if dataset == 0:\n",
        "      df=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\",header=None)\n",
        "      X=df.iloc[:,1:].to_numpy()\n",
        "      y=df[0].to_numpy()\n",
        "      b = np.zeros((y.size, y.max()+1))\n",
        "      b[np.arange(y.size),y] = 1\n",
        "      y=b      \n",
        "    \n",
        "    elif dataset == 1:\n",
        "     df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeTrain.csv\")\n",
        "\n",
        "    elif dataset == 2:\n",
        "     df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeValidation.csv\")\n",
        "    \n",
        "    return X, y\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFXETQq-Ago6"
      },
      "source": [
        "#My Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTMl6_XAFan"
      },
      "source": [
        "class MyNeuralNetwork():\n",
        "    \"\"\"\n",
        "    My implementation of a Neural Network Classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
        "    weight_inits = ['zero', 'random', 'normal']\n",
        "\n",
        "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
        "        \"\"\"\n",
        "        Initializing a new MyNeuralNetwork object\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers : int value specifying the number of layers\n",
        "\n",
        "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
        "\n",
        "        activation : string specifying the activation function to be used\n",
        "                     possible inputs: relu, sigmoid, linear, tanh\n",
        "\n",
        "        learning_rate : float value specifying the learning rate to be used\n",
        "\n",
        "        weight_init : string specifying the weight initialization function to be used\n",
        "                      possible inputs: zero, random, normal\n",
        "\n",
        "        batch_size : int value specifying the batch size to be used\n",
        "\n",
        "        num_epochs : int value specifying the number of epochs to be used\n",
        "        \"\"\"\n",
        "\n",
        "        if activation not in self.acti_fns:\n",
        "            raise Exception('Incorrect Activation Function')\n",
        "\n",
        "        if weight_init not in self.weight_inits:\n",
        "            raise Exception('Incorrect Weight Initialization Function')\n",
        "        \n",
        "        np.random.seed(0)\n",
        "        self.n_layers=n_layers\n",
        "        self.layer_sizes=layer_sizes \n",
        "        self.activation=activation \n",
        "        self.learning_rate=learning_rate \n",
        "        self.weight_init=weight_init\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        weights=[]\n",
        "        bias=[]\n",
        "        for i in range(self.n_layers-1):\n",
        "          weights.append(np.array(self.weight_func((self.layer_sizes[i],self.layer_sizes[i+1]))))\n",
        "          bias.append(np.zeros(self.layer_sizes[i+1])) \n",
        "\n",
        "        self.weights=np.array(weights)\n",
        "        self.bias=np.array(bias)\n",
        "        \n",
        "\n",
        "    def activation_func(self,X):\n",
        "      if self.activation==\"relu\":\n",
        "        return self.relu(X),self.relu_grad(X)\n",
        "      elif self.activation==\"sigmoid\":\n",
        "        return self.sigmoid(X),self.sigmoid_grad(X)\n",
        "      elif self.activation==\"linear\":\n",
        "        return self.linear(X),self.linear_grad(X)\n",
        "      elif self.activation==\"tanh\":\n",
        "        return self.tanh(X),self.tanh_grad(X)\n",
        "      else:\n",
        "        return self.softmax(X),self.softmax_grad(X)\n",
        "\n",
        "    def relu(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=np.maximum(0,X)\n",
        "        return x_calc\n",
        "\n",
        "    def relu_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        # print(X)\n",
        "        x_calc=np.zeros(X.shape)\n",
        "        x_calc[X>0]=1\n",
        "\n",
        "        return x_calc\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        x_calc= 1/(1+np.exp(-X))\n",
        "\n",
        "        return x_calc\n",
        "\n",
        "    def sigmoid_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        sig=self.sigmoid(X)\n",
        "        x_calc=sig(1-sig)\n",
        "        return x_calc\n",
        "\n",
        "    def linear(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=X\n",
        "        return x_calc\n",
        "\n",
        "    def linear_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=np.zeroes(X.shape)\n",
        "        x_calc[X>0]=1\n",
        "        x_calc[X<0]=-1\n",
        "        return x_calc\n",
        "\n",
        "    def tanh(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=(np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "        return x_calc\n",
        "\n",
        "    def tanh_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        tanh=self.tanh(X)\n",
        "        x_calc=1-tanh**2\n",
        "        return x_calc\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        expo = np.exp(X)\n",
        "        x_calc=expo/expo.sum()\n",
        "        return x_calc\n",
        "\n",
        "    def softmax_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Softmax activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        s = self.softmax(X)\n",
        "        si_sj = - s * s.reshape(X.shape[0], 1)\n",
        "        x_calc = np.diag(s) + si_sj\n",
        "        \n",
        "        return x_calc\n",
        "    \n",
        "\n",
        "    def weight_func(self,shape):\n",
        "      if self.weight_init==\"zero\":\n",
        "        return self.zero_init(shape)\n",
        "      elif self.weight_init==\"random\":\n",
        "        return self.random_init(shape)\n",
        "      else:\n",
        "        return self.normal_init(shape)\n",
        "\n",
        "    def zero_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Zero Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight= np.zeroes(shape)\n",
        "        return weight \n",
        "\n",
        "    def random_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Random Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight= np.random.rand(shape[0],shape[1])*10\n",
        "        return weight \n",
        "\n",
        "    def normal_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight=np.random.normal(size=shape)\n",
        "        return weight\n",
        "    \n",
        "    def cross_entropy(self,y_hat,y):\n",
        "        samples=y.shape[0]\n",
        "        error=y_hat-y\n",
        "        return error/samples\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fitting (training) the linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        self : an instance of self\n",
        "        \"\"\"\n",
        "        preActivation_H = {}\n",
        "        postActivation_H = {}\n",
        "\n",
        "        weights=self.weights\n",
        "        bias=self.bias\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "          for batch in range(0,X.shape[0],self.batch_size):\n",
        "            X_sample=X[batch:batch+self.batch_size,:]\n",
        "            y_sample=y[batch:batch+self.batch_size]\n",
        "            input=X_sample\n",
        "            output=y_sample\n",
        "\n",
        "            # Forward Propagation\n",
        "            for layer in range(self.n_layers-1):\n",
        "              layer_weight=weights[layer]\n",
        "              hidden_output=input.dot(layer_weight)+bias[layer]\n",
        "              hidden_output_activate,_=self.activation_func(hidden_output)          \n",
        "              input=hidden_output_activate  \n",
        "              preActivation_H[layer]=hidden_output     \n",
        "              postActivation_H[layer]=hidden_output_activate\n",
        "\n",
        "            \n",
        "            # Backward Propagation\n",
        "            dW=self.cross_entropy( postActivation_H[self.n_layers-2],output)\n",
        "            weights[self.n_layers-2]-=self.learning_rate*(postActivation_H[self.n_layers-3].T.dot(dW))\n",
        "            bias[self.n_layers-2]-=self.learning_rate*np.sum(dW)\n",
        "\n",
        "            \n",
        "            for layer in range(self.n_layers-3,0,-1):\n",
        "              delta=dW.dot(weights[layer+1].T)\n",
        "              _,derv=self.activation_func(preActivation_H[layer])\n",
        "              dW=delta*derv\n",
        "\n",
        "              weights[layer]-=self.learning_rate*postActivation_H[layer-1].T.dot(dW)\n",
        "              bias[layer]-=self.learning_rate*np.sum(dW,axis=0)\n",
        "\n",
        "            delta=dW.dot(weights[1].T)\n",
        "            _,derv=self.activation_func(preActivation_H[0])\n",
        "            dW=delta*derv\n",
        "            weights[0]-=self.learning_rate*X_sample.T.dot(dW)\n",
        "            bias[0]-=self.learning_rate*np.sum(dW,axis=0)\n",
        "\n",
        "        self.weights=weights\n",
        "        self.bias=bias\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicting probabilities using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the \n",
        "            class wise prediction probabilities.\n",
        "        \"\"\"\n",
        "        y=X\n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            z = np.dot(w, y) + b\n",
        "            y = self.activation_func(z)\n",
        "\n",
        "        # return the numpy array y which contains the probability of predicted values\n",
        "        return y\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "        \"\"\"\n",
        "        y=self.predict_proba(X)\n",
        "\n",
        "\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        return y.argmax()\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        acc : float value specifying the accuracy of the model on the provided testing set\n",
        "        \"\"\"\n",
        "\n",
        "        y_pred=self.predict(X)\n",
        "        acc=metrics.accuracy_score(y,y_pred)\n",
        "        return acc"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEVsh_lHOIiv"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RRakSxOs59",
        "outputId": "23b2a0f9-9ef9-414e-c01d-d1f1da791fcd"
      },
      "source": [
        "classifier=MyNeuralNetwork(4,[784,5,6,10],'relu',0.01,'random',6,100)\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MyNeuralNetwork at 0x7fcd5b3b5710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3GmRAYX5HzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a4aff6-151f-4fae-9093-762024c5d2c9"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(0)\n",
        "print(X.shape,y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 784) (20000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF-gm0-gU2nD",
        "outputId": "e07a057f-b6b1-486a-cf26-548b6fc1093c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dig = load_digits()\n",
        "onehot_target = pd.get_dummies(dig.target)\n",
        "x_train, x_val, y_train, y_val = train_test_split(dig.data, onehot_target, test_size=0.1, random_state=20)\n",
        "print(x_train.shape,y_train.shape)\n",
        "def sigmoid(s):\n",
        "    return 1/(1 + np.exp(-s))\n",
        "\n",
        "def sigmoid_derv(s):\n",
        "    return s * (1 - s)\n",
        "\n",
        "def softmax(s):\n",
        "    exps = np.exp(s - np.max(s, axis=1, keepdims=True))\n",
        "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(pred, real):\n",
        "    n_samples = real.shape[0]\n",
        "    res = pred - real\n",
        "    return res/n_samples\n",
        "\n",
        "def error(pred, real):\n",
        "    n_samples = real.shape[0]\n",
        "    logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
        "    loss = np.sum(logp)/n_samples\n",
        "    return loss\n",
        "\n",
        "class MyNN:\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        neurons = 128\n",
        "        self.lr = 0.5\n",
        "        ip_dim = x.shape[1]\n",
        "        op_dim = y.shape[1]\n",
        "\n",
        "        self.w1 = np.random.randn(ip_dim, neurons)\n",
        "        self.b1 = np.zeros((1, neurons))\n",
        "        self.w2 = np.random.randn(neurons, neurons)\n",
        "        self.b2 = np.zeros((1, neurons))\n",
        "        self.w3 = np.random.randn(neurons, op_dim)\n",
        "        self.b3 = np.zeros((1, op_dim))\n",
        "        self.y = y\n",
        "\n",
        "    def feedforward(self):\n",
        "        z1 = np.dot(self.x, self.w1) + self.b1\n",
        "        self.a1 = sigmoid(z1)\n",
        "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        self.a2 = sigmoid(z2)\n",
        "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
        "        self.a3 = softmax(z3)\n",
        "        \n",
        "    def backprop(self):\n",
        "        loss = error(self.a3, self.y)\n",
        "        # print('Error :', loss)\n",
        "        a3_delta = cross_entropy(self.a3, self.y) # w3\n",
        "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
        "        a2_delta = z2_delta * sigmoid_derv(self.a2) # w2\n",
        "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
        "        a1_delta = z1_delta * sigmoid_derv(self.a1) # w1\n",
        "        print(a3_delta.shape ,self.a3.shape, self.y.shape)\n",
        "        \n",
        "\n",
        "        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n",
        "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
        "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
        "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
        "        self.w1 -= self.lr * np.dot(self.x.T, a1_delta)\n",
        "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n",
        "        # print(self.b3.shape,self.b2.shape,self.b1.shape)\n",
        "\n",
        "    def predict(self, data):\n",
        "        self.x = data\n",
        "        self.feedforward()\n",
        "        return self.a3.argmax()\n",
        "\t\t\t\n",
        "model = MyNN(x_train/16.0, np.array(y_train))\n",
        "\n",
        "epochs = 12\n",
        "for x in range(epochs):\n",
        "    model.feedforward()\n",
        "    model.backprop()\n",
        "\t\t\n",
        "def get_acc(x, y):\n",
        "    acc = 0\n",
        "    for xx,yy in zip(x, y):\n",
        "        s = model.predict(xx)\n",
        "        if s == np.argmax(yy):\n",
        "            acc +=1\n",
        "    return acc/len(x)*100\n",
        "\t\n",
        "print(\"Training accuracy : \", get_acc(x_train/16, np.array(y_train)))\n",
        "print(\"Test accuracy : \", get_acc(x_val/16, np.array(y_val)))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1617, 64) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "(1617, 10) (1617, 10) (1617, 10)\n",
            "Training accuracy :  58.8126159554731\n",
            "Test accuracy :  56.666666666666664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ObObsq9bleh",
        "outputId": "3d93e37c-bdd3-4280-a2c7-a6da3828c9ba"
      },
      "source": [
        "b = np.zeros((y.size, y.max()+1))\n",
        "b[np.arange(y.size),y] = 1\n",
        "b.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATulQlTMEspt",
        "outputId": "bf828deb-1bee-473e-8976-59a818894247"
      },
      "source": [
        "b"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ3YqOcxFcEc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}