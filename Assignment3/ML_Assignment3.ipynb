{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1Tx4Gs0XAbV6",
        "CFx3VdqPNh9d"
      ],
      "mount_file_id": "1QJfFz7Aj86zDZsXOVQqodupeap6Mw6fM",
      "authorship_tag": "ABX9TyNmlfuulvrv1SQBhybiZNeB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/Assignment3/ML_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tx4Gs0XAbV6"
      },
      "source": [
        "#Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtBFGh_qAgIg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFx3VdqPNh9d"
      },
      "source": [
        "# Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBDQnce8Njuq"
      },
      "source": [
        "class MyPreProcessor():\n",
        "  \"\"\"\n",
        "  My steps for pre-processing for the All datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def pre_process(self, dataset):\n",
        "    \"\"\"\n",
        "    Reading the file and preprocessing the input and output.\n",
        "    Note that you will encode any string value and/or remove empty entries in this function only.\n",
        "    Further any pre processing steps have to be performed in this function too. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    dataset : integer with acceptable values 0, 1, or 2\n",
        "    0 ->  Dataset\n",
        "    1 ->  Dataset\n",
        "    2 ->  Dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    if dataset == 0:\n",
        "      df=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\",header=None)\n",
        "      X=df.iloc[:,1:].to_numpy()\n",
        "      X=scaler.fit_transform(X) \n",
        "\n",
        "      y=df[0].to_numpy()\n",
        "      b = np.zeros((y.size, y.max()+1))\n",
        "      b[np.arange(y.size),y] = 1\n",
        "      y=b\n",
        "           \n",
        "    \n",
        "    elif dataset == 1:\n",
        "      df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeTrain.csv\",header=None)\n",
        "      X=df.iloc[:,1:].to_numpy()\n",
        "      X=scaler.fit_transform(X) \n",
        "      y=df[0].to_numpy()\n",
        "      # b = np.zeros((y.size, y.max()+1))\n",
        "      # b[np.arange(y.size),y] = 1\n",
        "      # y=b\n",
        "\n",
        "    elif dataset == 2:\n",
        "      df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeValidation.csv\",header=None)\n",
        "      X=df.iloc[:,1:].to_numpy()\n",
        "      y=df[0].to_numpy()\n",
        "      # b = np.zeros((y.size, y.max()+1))\n",
        "      # b[np.arange(y.size),y] = 1\n",
        "      # y=b\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "preprocessor = MyPreProcessor()\n"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFXETQq-Ago6"
      },
      "source": [
        "#My Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTMl6_XAFan"
      },
      "source": [
        "class MyNeuralNetwork():\n",
        "    \"\"\"\n",
        "    My implementation of a Neural Network Classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
        "    weight_inits = ['zero', 'random', 'normal']\n",
        "\n",
        "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
        "        \"\"\"\n",
        "        Initializing a new MyNeuralNetwork object\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers : int value specifying the number of layers\n",
        "\n",
        "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
        "\n",
        "        activation : string specifying the activation function to be used\n",
        "                     possible inputs: relu, sigmoid, linear, tanh\n",
        "\n",
        "        learning_rate : float value specifying the learning rate to be used\n",
        "\n",
        "        weight_init : string specifying the weight initialization function to be used\n",
        "                      possible inputs: zero, random, normal\n",
        "\n",
        "        batch_size : int value specifying the batch size to be used\n",
        "\n",
        "        num_epochs : int value specifying the number of epochs to be used\n",
        "        \"\"\"\n",
        "\n",
        "        if activation not in self.acti_fns:\n",
        "            raise Exception('Incorrect Activation Function')\n",
        "\n",
        "        if weight_init not in self.weight_inits:\n",
        "            raise Exception('Incorrect Weight Initialization Function')\n",
        "        \n",
        "        np.random.seed(0)\n",
        "        self.n_layers=n_layers\n",
        "        self.layer_sizes=layer_sizes \n",
        "        self.activation=activation \n",
        "        self.learning_rate=learning_rate \n",
        "        self.weight_init=weight_init\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        weights=[]\n",
        "        bias=[]\n",
        "        for i in range(self.n_layers-1):\n",
        "          weights.append(np.array(self.weight_func((self.layer_sizes[i],self.layer_sizes[i+1]))))\n",
        "          bias.append(np.zeros(self.layer_sizes[i+1])) \n",
        "\n",
        "        self.weights=np.array(weights)\n",
        "        self.bias=np.array(bias)\n",
        "        \n",
        "\n",
        "    def activation_func(self,X):\n",
        "      if self.activation==\"relu\":\n",
        "        return self.relu(X),self.relu_grad(X)\n",
        "      elif self.activation==\"sigmoid\":\n",
        "        return self.sigmoid(X),self.sigmoid_grad(X)\n",
        "      elif self.activation==\"linear\":\n",
        "        return self.linear(X),self.linear_grad(X)\n",
        "      elif self.activation==\"tanh\":\n",
        "        return self.tanh(X),self.tanh_grad(X)\n",
        "      else:\n",
        "        return self.softmax(X),self.softmax_grad(X)\n",
        "\n",
        "    def relu(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=np.maximum(0,X)\n",
        "        return x_calc\n",
        "\n",
        "    def relu_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        # print(X)\n",
        "        x_calc=np.zeros(X.shape)\n",
        "        x_calc[X>0]=1\n",
        "\n",
        "        return x_calc\n",
        "\n",
        "    def sigmoid(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "\n",
        "        x_calc= 1/(1+np.exp(-X))\n",
        "\n",
        "        return x_calc\n",
        "\n",
        "    def sigmoid_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Sigmoid activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        sig=self.sigmoid(X)\n",
        "        x_calc=sig*(1-sig)\n",
        "        return x_calc\n",
        "\n",
        "    def linear(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=X\n",
        "        return x_calc\n",
        "\n",
        "    def linear_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Linear activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=np.zeros(X.shape)\n",
        "        x_calc[X>0]=1\n",
        "        x_calc[X<0]=-1\n",
        "        return x_calc\n",
        "\n",
        "    def tanh(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        x_calc=(np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
        "        return x_calc\n",
        "\n",
        "    def tanh_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Tanh activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        tanh=self.tanh(X)\n",
        "        x_calc=1-tanh**2\n",
        "        return x_calc\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the ReLU activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        expo = np.exp(X)\n",
        "        x_calc=expo/expo.sum(axis=0)\n",
        "        return x_calc\n",
        "\n",
        "    def softmax_grad(self, X):\n",
        "        \"\"\"\n",
        "        Calculating the gradient of Softmax activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 1-dimentional numpy array \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
        "        \"\"\"\n",
        "        s = self.softmax(X)\n",
        "        print(s.shape,X.shape)\n",
        "        si_sj = - s * s.reshape(X.shape[0], 1)\n",
        "        x_calc = np.diag(s) + si_sj\n",
        "        \n",
        "        return x_calc\n",
        "    \n",
        "\n",
        "    def weight_func(self,shape):\n",
        "      if self.weight_init==\"zero\":\n",
        "        return self.zero_init(shape)\n",
        "      elif self.weight_init==\"random\":\n",
        "        return self.random_init(shape)\n",
        "      else:\n",
        "        return self.normal_init(shape)\n",
        "\n",
        "    def zero_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Zero Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight= np.zeros(shape)\n",
        "        return weight \n",
        "\n",
        "    def random_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Random Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight= np.random.rand(shape[0],shape[1])*10\n",
        "        return weight \n",
        "\n",
        "    def normal_init(self, shape):\n",
        "        \"\"\"\n",
        "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
        "        \"\"\"\n",
        "        weight=np.random.normal(size=shape)\n",
        "        return weight\n",
        "    \n",
        "    def cross_entropy(self,y_hat,y):\n",
        "        samples=y.shape[0]\n",
        "        error=y_hat-y\n",
        "        return error/samples\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fitting (training) the linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        self : an instance of self\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        weights=self.weights\n",
        "        bias=self.bias\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "          for batch in range(0,X.shape[0],self.batch_size):\n",
        "            X_sample=X[batch:batch+self.batch_size,:]\n",
        "            y_sample=y[batch:batch+self.batch_size]\n",
        "            input=X_sample\n",
        "            output=y_sample\n",
        "            preActivation_H = {}\n",
        "            postActivation_H = {}\n",
        "\n",
        "            # Forward Propagation\n",
        "            for layer in range(self.n_layers-1):\n",
        "              layer_weight=weights[layer]\n",
        "              hidden_output=input.dot(layer_weight)+bias[layer]\n",
        "              hidden_output_activate,_=self.activation_func(hidden_output)          \n",
        "              input=hidden_output_activate  \n",
        "              preActivation_H[layer]=hidden_output     \n",
        "              postActivation_H[layer]=hidden_output_activate\n",
        "\n",
        "            \n",
        "            # Backward Propagation\n",
        "            dW=self.cross_entropy( postActivation_H[self.n_layers-2],output)\n",
        "            weights[self.n_layers-2]-=self.learning_rate*(postActivation_H[self.n_layers-3].T.dot(dW))\n",
        "            bias[self.n_layers-2]-=self.learning_rate*np.sum(dW)\n",
        "\n",
        "            for layer in range(self.n_layers-3,0,-1):\n",
        "              delta=dW.dot(weights[layer+1].T)\n",
        "              _,derv=self.activation_func(preActivation_H[layer])\n",
        "              dW=delta*derv\n",
        "\n",
        "              weights[layer]-=self.learning_rate*postActivation_H[layer-1].T.dot(dW)\n",
        "              bias[layer]-=self.learning_rate*np.sum(dW,axis=0)\n",
        "\n",
        "            delta=dW.dot(weights[1].T)\n",
        "            _,derv=self.activation_func(preActivation_H[0])\n",
        "            dW=delta*derv\n",
        "            weights[0]-=self.learning_rate*X_sample.T.dot(dW)\n",
        "            bias[0]-=self.learning_rate*np.sum(dW,axis=0)\n",
        "\n",
        "        self.weights=weights\n",
        "        self.bias=bias\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicting probabilities using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the \n",
        "            class wise prediction probabilities.\n",
        "        \"\"\"\n",
        "        y=X\n",
        "        for w, b in zip(self.weights, self.bias):\n",
        "            z = np.dot(y,w) + b\n",
        "            y,_ = self.activation_func(z)\n",
        "\n",
        "        # return the numpy array y which contains the probability of predicted values\n",
        "        return y\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "        \"\"\"\n",
        "        y=self.predict_proba(X)\n",
        "\n",
        "        # return the numpy array y which contains the predicted values\n",
        "        return y.argmax(axis=1)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Predicting values using the trained linear model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        acc : float value specifying the accuracy of the model on the provided testing set\n",
        "        \"\"\"\n",
        "\n",
        "        y_pred=self.predict(X)\n",
        "        y=y.argmax(axis=1)\n",
        "        acc=metrics.accuracy_score(y,y_pred)\n",
        "        return acc"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEVsh_lHOIiv"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo90cyqDhFNR",
        "outputId": "f5fe4f48-cce1-4e51-8ef8-f488edc04472"
      },
      "source": [
        "x_train= np.array([[1,2],[2,3]])\n",
        "y_train = np.array([[1,0],[0,1]])\n",
        "a=MyNeuralNetwork( 3,[2,5,2], \"sigmoid\", 0.01,\"zero\", 1,6)\n",
        "a.fit(x_train,y_train)\n",
        "a.predict(x_train)\n",
        "print(a.weights)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[3.80351334e-05, 3.80351334e-05, 3.80351334e-05, 3.80351334e-05,\n",
            "        3.80351334e-05],\n",
            "       [7.56573171e-05, 7.56573171e-05, 7.56573171e-05, 7.56573171e-05,\n",
            "        7.56573171e-05]])\n",
            " array([[-4.69244305e-05,  4.69244305e-05],\n",
            "       [-4.69244305e-05,  4.69244305e-05],\n",
            "       [-4.69244305e-05,  4.69244305e-05],\n",
            "       [-4.69244305e-05,  4.69244305e-05],\n",
            "       [-4.69244305e-05,  4.69244305e-05]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3GmRAYX5HzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66ae137-bbee-4c3c-94db-13a22294b099"
      },
      "source": [
        "X, y = preprocessor.pre_process(0)\n",
        "print(X.shape,y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 784) (20000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RRakSxOs59",
        "outputId": "81bdd6a4-aa4e-4a90-8d17-61c6be62214c"
      },
      "source": [
        "classifier=MyNeuralNetwork(5,[784,256, 128, 64,10],'linear',0.1,'random',600,50)\n",
        "classifier.fit(X_train,y_train)\n",
        "# print(classifier.weights)\n",
        "classifier.score(X_test,y_test)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:163: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in less\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.098"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b90wZXv4n05I",
        "outputId": "63fc55e1-1ac6-48a0-fd96-18dd9d6f49c9"
      },
      "source": [
        "classifier.predict(X_test)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:163: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in less\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_bsd1yBQYn9"
      },
      "source": [
        "# Q3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPNACnui3cx",
        "outputId": "2d3fa4c2-5cc6-484c-c4ba-4943896fb354"
      },
      "source": [
        "X_train,y_train= preprocessor.pre_process(1)\n",
        "X_val,y_val= preprocessor.pre_process(2)\n",
        "X_train.shape,y_train.shape,X_val.shape,y_val.shape"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9000, 128), (9000,), (1000, 128), (1000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UTKLXrflWi_"
      },
      "source": [
        "hidden_units=[5, 20, 50, 100 ,200]\n",
        "input_size=128\n",
        "hidden_sizes=5\n",
        "output_size=10\n",
        "BATCH_SIZE=600\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes, output_size),\n",
        "                      nn.Softmax(dim=1))"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwrWvudIzsAI"
      },
      "source": [
        "class MyDataset(data.Dataset):\n",
        "\n",
        "  def __init__(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    return self.X[i],self.y[i]\n",
        "  \n",
        "train_data=MyDataset(X_train,y_train)\n",
        "train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = BATCH_SIZE)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1qgkvf-gFQ5"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQfdS5Uy45Mh"
      },
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei1QAlBBu48e"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for (x, y) in iterator:\n",
        "        \n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        y_pred = model(x.float())\n",
        "        # print(y_pred)\n",
        "        \n",
        "        loss = criterion(y_pred, y)\n",
        "        \n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator),epoch_acc / len(iterator)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv6kfp68x325",
        "outputId": "d0335766-8e5b-4149-8b31-ada4ce64faa7"
      },
      "source": [
        "train(model,train_iterator,optimizer,criterion,device)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.297959836324056, 0.10255555609862009)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fAm1UOWylxS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}