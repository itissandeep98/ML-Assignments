{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNT8IQ1iaQM4WUIhxOfsP3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/ML_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5O3iEtAFqzd",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEo5gx2hZQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "%matplotlib inline"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9RJnOQAEAU3",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kS_U4I6EDez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyPreProcessor():\n",
        "  \"\"\"\n",
        "  My steps for pre-processing for the three datasets.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def pre_process(self, dataset):\n",
        "    \"\"\"\n",
        "    Reading the file and preprocessing the input and output.\n",
        "    Note that you will encode any string value and/or remove empty entries in this function only.\n",
        "    Further any pre processing steps have to be performed in this function too. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    dataset : integer with acceptable values 0, 1, or 2\n",
        "    0 -> Abalone Dataset\n",
        "    1 -> VideoGame Dataset\n",
        "    2 -> BankNote Authentication Dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "    \"\"\"     \n",
        "\n",
        "    if dataset == 0:\n",
        "      df=pd.read_csv('/content/Dataset.data',delim_whitespace=True,header=None) # data read from file\n",
        "      df.sample(frac=1) # data shuffled\n",
        "\n",
        "      # changed gender values to integers\n",
        "      df[0].replace('M',1,inplace=True)\n",
        "      df[0].replace('F',2,inplace=True)\n",
        "      df[0].replace('I',3,inplace=True)\n",
        "\n",
        "      data=df.to_numpy() # converted dataframe into numpy array\n",
        "      X=data[:,:-1]\n",
        "      y=data[:,-1]\n",
        "\n",
        "    elif dataset == 1:\n",
        "      df=pd.read_csv('/content/VideoGameDataset.csv') # data read from file\n",
        "      df=df[['Critic_Score','User_Score','Global_Sales']] # required colums extracted\n",
        "      df=df.sample(frac=1) # data shuffled\n",
        "\n",
        "\n",
        "      df['Critic_Score'].fillna(df['Critic_Score'].median(), inplace=True) # replaced NaN values with median of the column\n",
        "      df['User_Score'].replace(to_replace = 'tbd', value = np.nan,inplace=True) # replaced the cell with 'tbd' value to NaN value in the colum\n",
        "      df['User_Score']=df['User_Score'].astype(np.float) # converted column from strings to float values\n",
        "      df['User_Score'].fillna(df['User_Score'].median(), inplace=True) # replaced NaN values with median of the column\n",
        "\n",
        "      data=df.to_numpy() # converted dataframe into numpy array\n",
        "      X=data[:,:-1]\n",
        "      y=data[:,-1]\n",
        "\n",
        "    elif dataset == 2:\n",
        "      # Implement for the banknote authentication dataset\n",
        "      df=pd.read_csv('/content/data_banknote_authentication.txt',header=None) # data read from file\n",
        "      df=df.sample(frac=1) # data shuffled\n",
        "      X=df[[0,1,2,3]].to_numpy()\n",
        "      y=df[[4]].to_numpy()\n",
        "      y=y.squeeze()\n",
        "\n",
        "    elif dataset==3:\n",
        "      df=pd.read_csv('/content/Q4_Dataset.txt',delim_whitespace=True,header=None)\n",
        "      X=df[[1,2]].to_numpy()\n",
        "      y=df[[0]].to_numpy()\n",
        "\n",
        "    X=(X-X.mean(axis=0))/X.std(axis=0) # normalized the data \n",
        "\n",
        "    return X, y\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSTeIMNDSrEl",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofY4sQaGeYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLinearRegression():\n",
        "  \"\"\"\n",
        "\tMy implementation of Linear Regression.\n",
        "\t\"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def cross_validation(self,X,y,epoch=1000,alpha=0.01,k=10,lossfunc=1):\n",
        "    \"\"\"\n",
        "    performs k fold cross validation on the given dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) \n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    k : Number of folds the data needs to be splitted into\n",
        "\n",
        "    epoch : Number of times gradient descent has to run\n",
        "\n",
        "    alpha : Learning rate of gradient descent\n",
        "\n",
        "    lossfunc: determines which loss function to call\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=X.shape[0] # number of samples\n",
        "\n",
        "    split_start=0 # initial split's first index\n",
        "    split_end=m//k # initial split's last index\n",
        "\n",
        "    theta_list=[0]*k  # initialized theta\n",
        "    training_loss_list=[0]*k  # initialized list to store all the training loss from every fold\n",
        "    validation_loss_list=[0]*k # initialized list to store all the validation loss from every fold\n",
        "\n",
        "    error_min=float(\"inf\")\n",
        "    idx=0\n",
        "\n",
        "    for i in range(k):\n",
        "\n",
        "      # Extracting X and y for train and test set\n",
        "      X_train=np.concatenate((X[:split_start],X[split_end:]),axis=0)\n",
        "      y_train=np.concatenate((y[:split_start],y[split_end:]),axis=0)\n",
        "\n",
        "      X_test=X[split_start:split_end]\n",
        "      y_test=y[split_start:split_end]\n",
        "\n",
        "      self.fit(X_train,y_train,X_test,y_test,epoch,alpha,lossfunc) # calculating model parameters by running the gradient descent\n",
        "\n",
        "      # storing the results of current fold in the array\n",
        "      theta_list[i]=self.theta\n",
        "      training_loss_list[i]=self.training_loss\n",
        "      validation_loss_list[i]=self.validation_loss\n",
        "\n",
        "      split_start=split_end # updating slice parameters\n",
        "      split_end+=m//k\n",
        "\n",
        "      error=training_loss_list[i][-1]\n",
        "\n",
        "\n",
        "      # if the error in this fold is minimum of all errors seen upto now then update it and store the fold number\n",
        "      if(error<error_min):\n",
        "        idx=i\n",
        "        error_min=error\n",
        "    \n",
        "    # final storing the values associated with minimum error \n",
        "    self.theta=theta_list[idx]\n",
        "    self.training_loss=training_loss_list[idx]\n",
        "    self.validation_loss=validation_loss_list[idx]\n",
        "\n",
        "  def MSE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Mean Squared Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "\n",
        "    X_trans=np.transpose(X)                                 # Transpose of vector X\n",
        "    err=X.dot(theta)-y\n",
        "    derv =(1/m)*(X_trans.dot(err))                         # Calculates X` * ( X*theta - y )\n",
        "    \n",
        "    return derv,sum(err**2)/m    \n",
        " \n",
        "  def MAE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Mean Absolute Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "    err=(1/m)*(X.dot(theta)-y)          # Calculates (1/m) *( X*theta - y )\n",
        "    X_trans=X.T                         # Transpose of vector X\n",
        "    epsilon=10**-7\n",
        "    derv=(1/m)*(X_trans.dot(abs(err)/(err+epsilon)))\n",
        "\n",
        "    return derv,np.sum(abs(err))           # returns gradient and sum((1/m) *( |X*theta - y |))\n",
        "\n",
        "  def RMSE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Root Mean Squared Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) \n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "    X_trans=X.T                                          # Transpose of vector X\n",
        "    diff=X.dot(theta)-y                                  # Calculates ( X*theta - y )\n",
        "   \n",
        "    err=((1/m)*np.sum((diff)**2))**0.5                   # Calculates (1/m) *sqrt(sum((X*theta - y)^2))\n",
        "    derv =(1/m)*(X_trans.dot(diff))/err         \n",
        "    \n",
        "    return derv,err\n",
        "\n",
        "\n",
        "  def gradient_descent(self,X,y,X_test,y_test,epochs,alpha,lossfunc):\n",
        "    \"\"\"\n",
        "    Finding theta using the gradient descent method\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    X_test : 2-dimensional numpy array of shape (n_samples, n_features) which acts as Testing data.\n",
        "\n",
        "    y_test : 1-dimensional numpy array of shape (n_samples,) which acts as Testing labels.\n",
        "\n",
        "    epochs : Number of times gradient descent has to run\n",
        "\n",
        "    alpha : Learning rate of gradient descent\n",
        "\n",
        "    lossfunc: determines which loss function to call\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\n",
        "    training_loss: Calculated training loss at every theta\n",
        "\n",
        "    tvalidation_loss: Calculated validation loss at every theta\n",
        "    \"\"\"\n",
        "   \n",
        "    theta= np.zeros((X.shape[1],))                      # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "    \n",
        "    training_loss=np.array([])  # initializing array to store training loss at every value of theta\n",
        "    validation_loss= np.array([]) # initializing array to store validation loss at every value of theta\n",
        "\n",
        "\n",
        "    for i in range(epochs): \n",
        "      if(lossfunc==1): \n",
        "        derv,train_loss=self.RMSE(X,y,theta)   \n",
        "      elif(lossfunc==2):   \n",
        "        derv,train_loss=self.MAE(X,y,theta)\n",
        "      else:\n",
        "        derv,train_loss=self.MSE(X,y,theta)\n",
        "      training_loss=np.append(training_loss,train_loss)\n",
        "\n",
        "      \n",
        "\n",
        "      if(X_test is not None): # calculate validation loss only if test set is provided\n",
        "        if(lossfunc==1):\n",
        "          derv_val,val_loss=self.RMSE(X_test,y_test,theta)   \n",
        "        elif(lossfunc==2):   \n",
        "          derv_val,val_loss=self.MAE(X_test,y_test,theta)\n",
        "        else:\n",
        "          derv_val,val_loss=self.MSE(X_test,y_test,theta)\n",
        "\n",
        "        validation_loss=np.append(validation_loss,val_loss)\n",
        "\n",
        "      theta=theta-alpha*derv      \n",
        "  \n",
        "    return theta,training_loss,validation_loss\n",
        "\n",
        "  def fit(self, X, y,X_test=None,y_test=None,epoch=400,alpha=0.01,lossfunc=1):\n",
        "    \"\"\"\n",
        "    Fitting (training) the linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self : an instance of self\n",
        "    \"\"\"\n",
        "\n",
        "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1) # Adding a bias variable i.e columns of 1 to data\n",
        "\n",
        "    if(X_test is not None): # if validation set is provided then add a bias variable i.e columns of 1 to data\n",
        "      X_test=np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis=1)\n",
        "   \n",
        "    X_trans=np.transpose(X)\n",
        "    if(lossfunc==4):\n",
        "      try:\n",
        "        self.theta = np.linalg.inv(X_trans.dot(X)).dot(X_trans).dot(y)  # using the normal eqn, theta = inv(X`*X)*X`*y\n",
        "      except:\n",
        "        self.theta,self.training_loss,self.validation_loss = self.gradient_descent(X,y,X_test,y_test,epoch,alpha,lossfunc=1) # using the gradient descent method with RMSE loss function if the given data is non invertible\n",
        "    else: \n",
        "      self.theta,self.training_loss,self.validation_loss = self.gradient_descent(X,y,X_test,y_test,epoch,alpha,lossfunc) # using the gradient descent method with given number of epochs and learning rate\n",
        "      \n",
        "\n",
        "    # fit function has to return an instance of itself or else it won't work with test.py\n",
        "    return self\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predicting values using the trained linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "    \"\"\"\n",
        "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
        "    y=np.dot(X,self.theta)\n",
        "    # return the numpy array y which contains the predicted values\n",
        "    return y\n",
        "\n",
        "  def plot_loss(self):\n",
        "    print(\"Thetas:\",self.theta)\n",
        "    print(\"Training Loss:\",self.training_loss[-1])\n",
        "    print(\"Validation Loss:\",self.validation_loss[-1])\n",
        "    \n",
        "    x=np.arange(self.training_loss.shape[0])\n",
        "    plt.plot(x,self.training_loss,color=\"g\", label=\"Training Loss\")\n",
        "    plt.plot(x,self.validation_loss,color=\"r\",label=\"Validation Loss\")\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68z99ULqRBt",
        "colab_type": "text"
      },
      "source": [
        "## Dataset1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrZ20anXvGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(0)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(X,y,lossfunc=4)\n",
        "print(linear.theta)\n",
        "linear.cross_validation(X,y,epoch=10000,alpha=0.01,lossfunc=2)\n",
        "linear.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHOIS1Wb9tpj"
      },
      "source": [
        "## Dataset2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpSzdHFk9tpz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8e3a586-7886-4626-f8eb-3e9055228d82"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(1)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.cross_validation(X,y, epoch=10000,alpha=0.001, lossfunc=2)\n",
        "linear.plot_loss()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16719, 2) (16719,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU4kz7QRS5AO",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W66BTEUZGh4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLogisticRegression():\n",
        "\t\"\"\"\n",
        "\tMy implementation of Logistic Regression.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef sigmoid(self,z):\n",
        "\t\t\"\"\"\n",
        "\t\tFind the sigmoid value of z\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tz : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tvalue of z in sigmoid function\n",
        "\t\t\"\"\"\n",
        "\t\treturn 1/(1+np.exp(-z))\n",
        "\tdef accuracy(self,y_hat,y):\n",
        "\t\tm=y.shape[0]\n",
        "\t\ty_hat[y_hat>=0.5]=1\n",
        "\t\ty_hat[y_hat<0.5]=0\n",
        "\t\treturn (1-sum(abs(y-y_hat))/m)*100\n",
        "\n",
        "\tdef cost_diff(self,X,y,theta):\n",
        "\t\t\"\"\"\n",
        "\t\tFind Log Loss error in current model parameters\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\ttheta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tderv : derivative of cost at the value theta\n",
        "\t\t\"\"\"\n",
        "\t\tm=y.shape[0]\n",
        "\t\tX_trans=X.T\t\t  \t# Transpose of vector X\n",
        "\t\tz=X.dot(theta)\n",
        "\t\tactiv =self.sigmoid(z)\n",
        "\n",
        "\t\tderv =(X_trans.dot(activ-y))\t\t  # Calculates X` * ( sigmoid(X*theta) - y )\n",
        "\n",
        "\t\terr=(-1/m)*(np.sum(y*np.log(activ)+(1-y)*np.log(1-activ)))\n",
        "\t\n",
        "\t\taccuracy=self.accuracy(activ,y)\n",
        "\n",
        "\t\treturn derv,err,accuracy\n",
        "\n",
        "\n",
        "\tdef stochastic_gradient_descent(self,X,y,X_test=None,y_test=None,epoch=100,alpha=0.01):\n",
        "\t\t\"\"\"\n",
        "\t\tFinding theta using the stochastic gradient descent model\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tepochs : Number of times gradient descent has to run\n",
        "\n",
        "\t\talpha : Learning rate of gradient descent\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ttheta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\t\t\"\"\"\n",
        "\t\tm=y.shape[0]\n",
        "\t\t\n",
        "\t\ttheta= np.zeros((X.shape[1],))\t\t  # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "\t\t\n",
        "\t\tvalidation_loss_list=np.array([])\n",
        "\t\ttraining_loss_list=np.array([])\n",
        "\t\n",
        "\t\tvalidation_acc_list=np.array([])\n",
        "\t\ttraining_acc_list=np.array([])\n",
        "\n",
        "\t\tfor i in range(epoch):\n",
        "\t\t\tk=randint(0,m-1)\n",
        "\t\t\tcurr_X=np.array([X[k]])\n",
        "\t\t\tcurr_y=np.array([y[k]])\n",
        "\t\t\tderv,loss,accuracy=self.cost_diff(curr_X,curr_y,theta)\n",
        "\t\t\tderv_train,loss_train,accuracy_train=self.cost_diff(X,y,theta)\n",
        "\t\t\ttraining_loss_list=np.append(training_loss_list,loss_train)\n",
        "\t\t\ttraining_acc_list=np.append(training_acc_list,accuracy_train)\n",
        "\n",
        "\t\t\tif(X_test is not None):\n",
        "\t\t\t\tderv_val,loss_val,accuracy_val=self.cost_diff(X_test,y_test,theta)\n",
        "\t\t\t\tvalidation_loss_list=np.append(validation_loss_list,loss_val)\n",
        "\t\t\t\tvalidation_acc_list=np.append(validation_acc_list,accuracy_val)\n",
        "\t\t\n",
        "\t\t\ttheta=theta-(alpha/m)*derv\n",
        "\t\t\n",
        "\t\treturn theta,validation_loss_list,training_loss_list,validation_acc_list,training_acc_list\n",
        "\n",
        "\n",
        "\t\n",
        "\tdef batch_gradient_descent(self,X,y,X_test=None,y_test=None,theta=None,epoch=1000,alpha=0.01):\n",
        "\t\t\"\"\"\n",
        "\t\tFinding theta using the batch gradient descent model\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tepochs : Number of times gradient descent has to run\n",
        "\n",
        "\t\talpha : Learning rate of gradient descent\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ttheta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\t\t\"\"\"\n",
        "\t\tm=y.shape[0]\n",
        "\t\t\n",
        "\t\ttheta= np.zeros((X.shape[1],))\t\t  # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "\t\t\n",
        "\t\tvalidation_loss_list=np.array([])\n",
        "\t\ttraining_loss_list=np.array([])\n",
        "\t\n",
        "\t\tvalidation_acc_list=np.array([])\n",
        "\t\ttraining_acc_list=np.array([])\n",
        "\n",
        "\t\tfor i in range(epoch):\n",
        "\t\t\tderv_train,loss_train,accuracy_train=self.cost_diff(X,y,theta)\n",
        "\t\t\ttraining_loss_list=np.append(training_loss_list,loss_train)\n",
        "\t\t\ttraining_acc_list=np.append(training_acc_list,accuracy_train)\n",
        "\n",
        "\t\t\tif(X_test is not None):\n",
        "\t\t\t\tderv_val,loss_val,accuracy_val=self.cost_diff(X_test,y_test,theta)\n",
        "\t\t\t\tvalidation_loss_list=np.append(validation_loss_list,loss_val)\n",
        "\t\t\t\tvalidation_acc_list=np.append(validation_acc_list,accuracy_val)\n",
        "\t\t\n",
        "\t\t\ttheta=theta-(alpha/m)*derv_train\n",
        "\t\t\n",
        "\t\treturn theta,validation_loss_list,training_loss_list,validation_acc_list,training_acc_list\n",
        "\n",
        "\tdef fit(self, X, y,X_test=None,y_test=None,epoch=10000,alpha=0.01,type=0):\n",
        "\t\t\"\"\"\n",
        "\t\tFitting (training) the logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tself : an instance of self\n",
        "\t\t\"\"\"\n",
        "\t\tX=np.concatenate((np.ones((X.shape[0],1)),X),axis=1) # Adding a bias variable i.e columns of 1 to data\n",
        "\n",
        "\t\tif(X_test is not None): # if validation set is provided then add a bias variable i.e columns of 1 to data\n",
        "\t\t\tX_test=np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis=1)\n",
        "\t\tif(type==0):\n",
        "\t\t\tself.theta,self.validation_loss,self.training_loss,self.validation_acc,self.training_acc = self.batch_gradient_descent(X,y,X_test,y_test,epoch,alpha)\t\t  # using the gradient descent method with given number of epochs and learning rate\n",
        "\t\telse:\n",
        "\t\t\tself.theta,self.validation_loss,self.training_loss,self.validation_acc,self.training_acc = self.stochastic_gradient_descent(X,y,X_test,y_test,epoch,alpha)\n",
        "\t\t\n",
        "\t\t# fit function has to return an instance of itself or else it won't work with test.py\n",
        "\t\treturn self\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\t\"\"\"\n",
        "\t\tPredicting values using the trained logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "\t\t\"\"\"\n",
        "\t\tX=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
        "\t\ty=self.sigmoid(X.dot(self.theta))\n",
        "\t\n",
        "\t\ty[y>=0.5]=1\n",
        "\t\ty[y<0.5]=0\n",
        "\n",
        "\t\t# return the numpy array y which contains the predicted values\n",
        "\t\treturn y\n",
        "\n",
        "\tdef plot_loss(self):\n",
        "\t\tprint(\"Thetas:\",self.theta)\n",
        "\t\tprint(\"Training Loss:\",self.training_loss[-1])\n",
        "\t\tprint(\"Validation Loss:\",self.validation_loss[-1])\n",
        "\n",
        "\t\tx=np.arange(self.training_loss.shape[0])\n",
        "\t\tplt.plot(x,self.training_loss,color=\"g\", label=\"Training Loss\")\n",
        "\t\tplt.plot(x,self.validation_loss,color=\"r\",label=\"Validation Loss\")\n",
        "\t\tplt.xlabel('Iteration')\n",
        "\t\tplt.ylabel('Loss')\n",
        "\t\tplt.legend()\n",
        "\t\tplt.show()\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erIgoN4nvyyr",
        "colab_type": "text"
      },
      "source": [
        "## Dataset 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYa7lf2fv11j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "4c51221a-dbb6-4d87-ad77-b74a5c7e8870"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(2)\n",
        "X_test=X[:X.shape[0]//8]\n",
        "y_test=y[:y.shape[0]//8]\n",
        "\n",
        "X_train=X[X.shape[0]//8:]\n",
        "y_train=y[y.shape[0]//8:]\n",
        "\n",
        "logistic = MyLogisticRegression()\n",
        "logistic.fit(X_train, y_train,X_test,y_test,alpha=10,type=1)\n",
        "print(logistic.training_acc[-1],logistic.validation_acc[-1])\n",
        "logistic.plot_loss()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97.58534554537886 97.6608187134503\n",
            "Thetas: [-0.62939485 -3.33535708 -2.76254452 -2.45967134  0.25816338]\n",
            "Training Loss: 0.08926608677982482\n",
            "Validation Loss: 0.0969323655507881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xV5f3A8c83e++wkkAAGYYNAUREoC5UlDqwIFbQOn9Oal2tv0pRf9VK1eIsdU9UWhEUxYlYUSAgIJsQAoSZBJJASMh6fn88J8kFAiQhNzfJ/b5fr/u65zxn3O/JhXzznPMMMcaglFLKe/l4OgCllFKepYlAKaW8nCYCpZTycpoIlFLKy2kiUEopL+fn6QDqKi4uziQnJ3s6DKWUalaWLVuWY4yJr2lbs0sEycnJpKWleToMpZRqVkRk6/G26a0hpZTycpoIlFLKy2kiUEopL+fWZwQiMgr4B+ALvGyMefyo7U8DI53VEKCVMSbKnTEppWqntLSUrKwsiouLPR2KqoOgoCASExPx9/ev9TFuSwQi4gs8D5wHZAFLRWSOMWZt5T7GmMku+98B9HNXPEqpusnKyiI8PJzk5GRExNPhqFowxpCbm0tWVhYdO3as9XHuvDU0CEg3xmQYY0qAmcCYE+w/HnjPjfEopeqguLiY2NhYTQLNiIgQGxtb51qcOxNBArDdZT3LKTuGiHQAOgLfHGf7TSKSJiJp2dnZDR6oUqpmmgSan/p8Z03lYfE4YJYxprymjcaYGcaYVGNManx8jf0hTmrlf17ku6uHUlZeeipxKqVUi+PORLADSHJZT3TKajION98W2rfgM4a/t4iiPccLQSnVlOTm5tK3b1/69u1LmzZtSEhIqFovKSk54bFpaWnceeedJ/2MM888s0FiXbBgAaNHj26Qc3mCO1sNLQW6iEhHbAIYB1x99E4i0h2IBn50YyxURNvGSIf37SW8XbI7P0op1QBiY2NZsWIFAFOmTCEsLIw//OEPVdvLysrw86v5V1hqaiqpqakn/YxFixY1TLDNnNtqBMaYMuB2YD6wDvjAGLNGRKaKyKUuu44DZho3T5UmkZEAlOzPcefHKKXcaNKkSdxyyy0MHjyY++67jyVLljBkyBD69evHmWeeyYYNG4Aj/0KfMmUK119/PSNGjKBTp05Mnz696nxhYWFV+48YMYIrr7yS7t27M2HCBCp/Jc2bN4/u3bszYMAA7rzzzjr95f/ee+/Rq1cvevbsyf333w9AeXk5kyZNomfPnvTq1Yunn34agOnTp5OSkkLv3r0ZN27cqf+w6sCt/QiMMfOAeUeV/fmo9SnujKGSX1QMAMX79jbGxynVotz9+d2s2L2iQc/Zt01fnhn1TJ2Py8rKYtGiRfj6+lJQUMD333+Pn58fX331FX/84x/597//fcwx69ev59tvv+XAgQN069aNW2+99Zh29j///DNr1qyhXbt2DB06lB9++IHU1FRuvvlmFi5cSMeOHRk/fnyt49y5cyf3338/y5YtIzo6mvPPP5/Zs2eTlJTEjh07WL16NQB5eXkAPP7442zZsoXAwMCqssbSVB4Wu11lItAagVLN29ixY/H19QUgPz+fsWPH0rNnTyZPnsyaNWtqPObiiy8mMDCQuLg4WrVqxZ49e47ZZ9CgQSQmJuLj40Pfvn3JzMxk/fr1dOrUqapNfl0SwdKlSxkxYgTx8fH4+fkxYcIEFi5cSKdOncjIyOCOO+7g888/JyIiAoDevXszYcIE3n777ePe8nKXZjf6aH0FRtvWRqV5+zwciVLNT33+cneX0NDQquX//d//ZeTIkXz00UdkZmYyYsSIGo8JDAysWvb19aWsrKxe+zSE6OhoVq5cyfz583nppZf44IMPePXVV/n0009ZuHAhc+fO5bHHHuOXX35ptITgNTWCgBibCCr2ayJQqqXIz88nIcF2T3r99dcb/PzdunUjIyODzMxMAN5///1aHzto0CC+++47cnJyKC8v57333mP48OHk5ORQUVHBFVdcwaOPPsry5cupqKhg+/btjBw5kieeeIL8/HwOHjzY4NdzPF5TIwiObwdA8ptz4C8veTgapVRDuO+++5g4cSKPPvooF198cYOfPzg4mBdeeIFRo0YRGhrKwIEDj7vv119/TWJiYtX6hx9+yOOPP87IkSMxxnDxxRczZswYVq5cyXXXXUdFRQUAf/3rXykvL+eaa64hPz8fYwx33nknUVGNN+yauLmxToNLTU019ZmYZteBXbSNsMmAZnbNSnnCunXrOP300z0dhscdPHiQsLAwjDHcdtttdOnShcmTJ5/8QA+q6bsTkWXGmBrb1HrNraGwgDDyAk++n1JKufrXv/5F37596dGjB/n5+dx8882eDqnBec2toRD/EL5OgPMzgIoK8PGaHKiUOgWTJ09u8jWAU+U1vw19fXzZGe6s7Nzp0ViUUqop8ZpEAPBOb2dh82aPxqGUUk2JVyWCnmfYkS1MerqHI1FKqabDqxLB7mj7SGT3xuUejkQppZoOr0oEN51xO/mBULQny9OhKKVOYuTIkcyfP/+IsmeeeYZbb731uMeMGDGCyublF110UY1j9kyZMoVp06ad8LNnz57N2rVVs+ry5z//ma+++qou4deoqQ5X7VWJoEtsF3JCoHzvseOMKKWalvHjxzNz5swjymbOnFnr8X7mzZtX705ZRyeCqVOncu6559brXM2BVyWC+JB4coPBZ58OM6FUU3fllVfy6aefVk1Ck5mZyc6dOxk2bBi33norqamp9OjRg4cffrjG45OTk8nJsYNMPvbYY3Tt2pWzzjqraqhqsH0EBg4cSJ8+fbjiiis4dOgQixYtYs6cOdx777307duXzZs3M2nSJGbNmgXYHsT9+vWjV69eXH/99Rw+fLjq8x5++GH69+9Pr169WL9+fa2v1dPDVXtNPwKAAN8AckOhXV7jjeGhVItw992womGHoaZvX3jm+IPZxcTEMGjQID777DPGjBnDzJkzueqqqxARHnvsMWJiYigvL+ecc85h1apV9O7du8bzLFu2jJkzZ7JixQrKysro378/AwYMAODyyy/nxhtvBOChhx7ilVde4Y477uDSSy9l9OjRXHnllUecq7i4mEmTJvH111/TtWtXrr32Wl588UXuvvtuAOLi4li+fDkvvPAC06ZN4+WXXz7pj6EpDFftVTUCESG43JfEDbs8HYpSqhZcbw+53hb64IMP6N+/P/369WPNmjVH3MY52vfff89ll11GSEgIERERXHpp9bxYq1evZtiwYfTq1Yt33nnnuMNYV9qwYQMdO3aka9euAEycOJGFCxdWbb/88ssBGDBgQNVAdSfTFIar9qoaAYCPjy9QDmVl0MhjfivVbJ3gL3d3GjNmDJMnT2b58uUcOnSIAQMGsGXLFqZNm8bSpUuJjo5m0qRJFBcX1+v8kyZNYvbs2fTp04fXX3+dBQsWnFK8lUNZN8Qw1o05XLVX1QgA5g+wWZVdWitQqqkLCwtj5MiRXH/99VW1gYKCAkJDQ4mMjGTPnj189tlnJzzH2WefzezZsykqKuLAgQPMnTu3atuBAwdo27YtpaWlvPPOO1Xl4eHhHDhw4JhzdevWjczMTNKdvkhvvfUWw4cPP6VrbArDVXvdn8RZUU7u27oVkpI8G4xS6qTGjx/PZZddVnWLqE+fPvTr14/u3buTlJTE0KFDT3h8//79+c1vfkOfPn1o1arVEUNJP/LIIwwePJj4+HgGDx5c9ct/3Lhx3HjjjUyfPr3qITFAUFAQr732GmPHjqWsrIyBAwdyyy231Ol6muJw1V4zDHWlC//cmc8eyYDXX4eJExsuMKVaGB2GuvnSYahPou8gZ5iJbds8HIlSSjUNXpcIWsd1YE8oHN6i4w0ppRR4YyIIbc22SCjL1BFIlTqZ5nbrWNXvO/O6RNAmrA1bI0G2bfd0KEo1aUFBQeTm5moyaEaMMeTm5hIUFFSn49zaakhERgH/AHyBl40xj9ewz1XAFMAAK40xV7szptZhrVkZAQGrs935MUo1e4mJiWRlZZGdrf9XmpOgoKAjWiXVhtsSgYj4As8D5wFZwFIRmWOMWeuyTxfgQWCoMWa/iLRyVzyVWoe2Zmc4+BcWwYEDEB5+8oOU8kL+/v507NjR02GoRuDOW0ODgHRjTIYxpgSYCYw5ap8bgeeNMfsBjDF73RgPANHB0dVTVmqnMqWUcmsiSABcb8RnOWWuugJdReQHEfnJuZV0DBG5SUTSRCTtVKupPuLDnjBnZdWqUzqXUkq1BJ5+WOwHdAFGAOOBf4nIMd3kjDEzjDGpxpjU+Pj4U/7Q3t3OtgsNMNGEUko1d+5MBDsA1zEcEp0yV1nAHGNMqTFmC7ARmxjcKr97MuUCtG7t7o9SSqkmz52JYCnQRUQ6ikgAMA6Yc9Q+s7G1AUQkDnurKMONMQEQFhrN7nCBLVvc/VFKKdXkuS0RGGPKgNuB+cA64ANjzBoRmSoilQOCzwdyRWQt8C1wrzEm110xVYoKiiKhwMBbb7n7o5RSqslzaz8CY8w8YN5RZX92WTbA751Xo4kKOvXR+pRSqqXw9MNij4gJjuF/RzorznyjSinlrbwyESRGJLKrsgnp7t0ejUUppTzNKxNB+8j27KrsVKaJQCnl5bwyEbQNa8vuyhqB9i5WSnk5r0wEoQGhHIgJtSs7d3o2GKWU8jCvTAQA6QGFFPkBGW7vtqCUUk2a1yYC4wObo4FNmzwdilJKeZTXJoIrTr+CTbFgNBEopbyc1yaCM5POZFMMsHkzlJd7OhyllPIYr00EscGxbIoFKSmB7TptpVLKe3ltIogLiSM9xlnZrBPZK6W8l9cmgtiQWDIrhxx66CGPxqKUUp7ktYmge1x3tkc6Kz/95NFYlFLKk9w6+mhTFhUURUJ0e1b3LqFnSLKnw1FKKY/x2hoBQMeojmyJMtqpTCnl1bw6EXSI6sCasCLYuxcOHvR0OEop5RHenQgiO/BzSIFd0WkrlVJeyqsTQefozmyMdVY2bPBoLEop5SlenQi6x3VnQywYEVizxtPhKKWUR3h1Ijgt5jSKAiCvXQysXevpcJRSyiO8OhHEBMcQFxJHRlIYLFni6XCUUsojvDoRiAgp8SmkJQhkZmrLIaWUV/LqRACw88BOviXTrixa5NFYlFLKE9yaCERklIhsEJF0EXmghu2TRCRbRFY4rxvcGU9Nzut0HmvjnRVtOaSU8kJuSwQi4gs8D1wIpADjRSSlhl3fN8b0dV4vuyue47m2z7WsbgVlIUE6W5lSyiu5s0YwCEg3xmQYY0qAmcAYN35evbQObY3xgf3JbWDxYk+Ho5RSjc6diSABcJ3xJcspO9oVIrJKRGaJSFJNJxKRm0QkTUTSsrOzGzTI1mGtASgoPWhbDlVUNOj5lVKqqfP0w+K5QLIxpjfwJfBGTTsZY2YYY1KNManx8fE17VJvIf4hALyZkGMLMjMb9PxKKdXUuTMR7ABc/8JPdMqqGGNyjTGHndWXgQFujOeE5nZ1Fn74wVMhKKWUR7gzESwFuohIRxEJAMYBc1x3EJG2LquXAuvcGM9xTR0xlRVtoCI2Br75xhMhKKWUx7htYhpjTJmI3A7MB3yBV40xa0RkKpBmjJkD3CkilwJlwD5gkrviOZF24e0wPrCzVzKJ2pdAKeVl3DpDmTFmHjDvqLI/uyw/CDzozhhqY0TyCAC2dowh8bufobAQQkM9G5RSSjUSTz8sbhI6RXcC4MdWh8EYWL7cwxEppVTj0USAHXMI4K/me1vw3XcejEYppRqXJgLH0KSh7AsBevfWRKCU8iqaCByXdb8MgKKhg20T0pISD0eklFKNQxOBIzbEzln57/gcKCqCtDQPR6SUUo1DE4Fj1GmjAFjRNcIWLFjguWCUUqoRaSJwtAlrQ6vQVuSF+0Hr1jB9uqdDUkqpRqGJwEWXmC5szN0ISUmwZ48OQKeU8gqaCFz0bt2blXtWUnHb/9iC1as9G5BSSjUCTQQuBrQdQMHhAramdrEFTz3l2YCUUqoRaCJwkRJvJ1BbG5BvC+bP92A0SinVODQRuOgW1w2ALzZ/AWecAbt3Q06Oh6NSSin30kTgIiY4BoDpS6bDn/5kC3U0UqVUC6eJ4CjhAeEAmF/9Cvz9NREopVo8TQRHefK8JwHIPLwH+vfXRKCUavE0ERxlQDs7W+ayXcvgrLNg8WLIz/dwVEop5T6aCI7Sq1UvAnwDWJy1GK66yg4+N2uWp8NSSim30URwlEC/QErKS5j24zQYOBCCg+GGG+yENUop1QJpIqhBYkQiAJn5W+1zAoBVqzwYkVJKuY8mghp8PO5jAOZtmgcvvWQLL7jAgxEppZT7aCKoQb82/QC4bd5t0LOnLdyzBwoKPBiVUkq5hyaCGogIgxMGA1BcVlzdhHTOHA9GpZRS7lGrRCAioSLi4yx3FZFLRcTfvaF51gNnPQBA2s40GDzYDk39wQcejkoppRpebWsEC4EgEUkAvgB+C7zurqCaguEdhgMw7LVh4OMDKSkwd64OTa2UanFqmwjEGHMIuBx4wRgzFuhx0oNERonIBhFJF5EHTrDfFSJiRCS1lvG4XXRwdNXy9vzt8MgjdkX7FCilWphaJwIRGQJMAD51ynxPcoAv8DxwIZACjBeRlBr2CwfuAhbXNujGMu/qeQAs2r7I9ilISYGpUz0clVJKNazaJoK7gQeBj4wxa0SkE/DtSY4ZBKQbYzKMMSXATGBMDfs9AjwBFNcylkZzbqdzAXh4wcO2oG9f27Fs2TIPRqWUUg2rVonAGPOdMeZSY8wTzkPjHGPMnSc5LAHY7rKe5ZRVEZH+QJIx5lNOQERuEpE0EUnLzs6uTcgNwt/XHz8fPzbkbsAYA08/bTfMm9doMSillLvVttXQuyISISKhwGpgrYjceyof7CSUp4B7TravMWaGMSbVGJMaHx9/Kh9bZ9f1vQ6AuRvnQqtW0KcPfPZZo8aglFLuVNtbQynGmALg18BnQEdsy6ET2QEkuawnOmWVwoGewAIRyQTOAOY0pQfGAE+c+wQAr/z8ii2YMAF+/BHWrfNgVEop1XBqmwj8nX4DvwbmGGNKgZONwrYU6CIiHUUkABgHVPXIMsbkG2PijDHJxphk4CfgUmNMWp2vwo2ig6Pp1aoXi7MW29tD114Lfn7wyiueDk0ppRpEbRPBP4FMIBRYKCIdgBOOt2CMKQNuB+YD64APnAfNU0Xk0vqH3PjuGHQHewr3sHrvamjdGi65BN580w5RrZRSzVxtHxZPN8YkGGMuMtZWYGQtjptnjOlqjOlsjHnMKfuzMeaYsRqMMSOaWm2g0iXdLgFgzgYn7BtugOxseP11zwWllFINpLYPiyNF5KnKljsi8nds7cArtAlrQ/vI9jz07UO2oHIk0ptvtglBKaWasdreGnoVOABc5bwKgNfcFVRT1KtVLwCW71oOvr7w7rt2w6OPejAqpZQ6dbVNBJ2NMQ87ncMyjDF/ATq5M7Cm5qXRdl6C91e/bwvGj4fLL4fp0+GnnzwYmVJKnZraJoIiETmrckVEhgJF7gmpaUqMSOT8zucza90s23oI4O9/t+9DhkBpqeeCU0qpU1DbRHAL8LyIZDpt/p8DbnZbVE3U2JSxZOzP4OfdP9uC5GS4/367fOutHotLKaVORW1bDa00xvQBegO9jTH9gF+5NbIm6LLul+Hv48/rK16vLnzsMfv+yivanFQp1SzVaYYyY0yB08MY4PduiKdJiw2JZWyPsby16i0OlR6yhb6+dp4CgDE1jamnlFJN26lMVSkNFkUzclP/m8grzuPDNR9WF158sX3//HPIyPBMYEopVU+nkghONsREi3R2h7Px8/Fj0seTKKsos4Ui1WMP3Xef54JTSql6OGEiEJEDIlJQw+sA0K6RYmxSRITbB94OwBsr3qje0L07PPAA/PvfMGWKZ4JTSql6kKqmkM1EamqqSUvz7EgUFaYC36m+nJF4Bj/+7sfqDXl5EB1tB6XbsgUSEz0XpFJKuRCRZcaYGkd3PpVbQ17LR3yYPmo6P2X9xJIdS6o3REXZye3LyiApCfbu9VyQSilVS5oI6mli34mEB4Tz7JJnj9zQowdMm2aXW7eGIq/qd6eUaoY0EdRTRGAE1/W9jvdXv8+uA7uO3HiPy6RrDz7YuIEppVQdaSI4BXcMvoOyijKmL55+7MbKISf+8Q/IyWncwJRSqg40EZyC02JOY2yPsTy39Dn2Fh71PMDPr3owuqlTGz84pZSqJU0Ep2jqiKkUlxUzZcGUYzcOHgyTJsFzz0F6emOHppRStaKJ4BR1i+vGLQNuYcayGXYqy6M99JAdhiIlBYqLGz9ApZQ6CU0EDWDKiClEBEYwef5kjumX0bkz3HmnfWZwww2eCVAppU5AE0EDiA2J5eHhD/NVxld8uunTY3f4+9/ttJbvvAMPP9z4ASql1Aloz+IGUlpeSu+XelNUWsTq/1lNWEDYkTscOgShzjTPO3ZAO68coUMp5SHas7gR+Pv6M2P0DLbmb2XqdzW0EgoJgV9+scuDBkEzS8BKqZZLE0EDGtZhGDf1v4knFz3J/PT5x+7Qs6ed63jHDrjttsYPUCmlaqCJoIE9M+oZusd157Z5t1FcVkMrobffht694cUX7TMDpZTyMLcmAhEZJSIbRCRdRB6oYfstIvKLiKwQkf+KSIo742kMwf7BPHvhs2zev5l7v7j32B18fGDOHLt8zTXw3/82boBKKXUUtyUCEfEFngcuBFKA8TX8on/XGNPLGNMX+BvwlLviaUzndjqXuwbfxXNLn+PTjTW0IurQATZtssvDhkFmZqPGp5RSrtxZIxgEpBtjMowxJcBM4IhJfV3mPwYIpQXNevb4uY/Tu3VvJn08iR0FO47d4bTT7G0igAEDYPv2xg1QKaUc7kwECYDrb7csp+wIInKbiGzG1gjurOlEInKTiKSJSFp2drZbgm1oQX5BvH/l+xwqPcS1s6+lwlQcu9OECfY5wb590L49zJrV+IEqpbyexx8WG2OeN8Z0Bu4HHjrOPjOMManGmNT4+PjGDfAUdI/rzvRR0/lmyzc8uvDRmne6+mqYN88uX3utneVMKaUakTsTwQ4gyWU90Sk7npnAr90Yj0dc3+96rul9DQ8veJh3Vh2nldCFF8IHH9hJbKKjYdmyxg1SKeXV3JkIlgJdRKSjiAQA44A5rjuISBeX1YuBTW6MxyNEhJcveZnhHYYzcfZEPk//vOYdx46tnvQ+NRXWrWu0GJVS3s1ticAYUwbcDswH1gEfGGPWiMhUEbnU2e12EVkjIiuA3wMT3RWPJwX6BTJ3/Fx6t+7NlR9cybKdx/mL/+GH4emn7XJKCqxZ03hBKqW8lo411Ih2HdjFkFeG4Ovjy/KblhMZFFnzjjNn2h7IAN9/D2ed1XhBKqVaJB1rqIloG96WNy97k23527ju4+uOHbK60rhx8N57dnnYsOoxipRSyg00ETSyszuczf/96v/4aP1HPLvk2ePvOG4crFhhl3v31kHqlFJuo4nAA+458x7GdBvD5PmTj9+SCKBPn+pbRB991DjBKaW8jiYCD/ARH96+/G2GdxjOtbOv5fUVrx9/55degm7d4Ior4O67tWaglGpwmgg8JCwgjLnj53J2h7O5ae5NzFp7nF7FERGwcKGdyOYf/4Df/Q4OH27cYJVSLZomAg8KDQhl1thZpLZLZeyHY3lh6Qs179iqFWRlwR/+AK+9BkFBtmWRUko1AE0EHhYbEss3E79hdNfR3PHZHXyy8ZOadxSBJ5+EBx+06+PH245nBQU176+UUrWkiaAJCPILYuYVM+nTug+Xv3857/7y7vF3/r//g4MH4bzz7FAU55wD776rzw6UUvWmiaCJCA0I5ZuJ3zC0/VB++9FvmbFsxgl2DoUvvrB9DVautKOY9ukD//wnlJU1XtBKqRZBE0ETEhUUxadXf8p5nc7j5k9uZvLnkymrOMEv9nHjYM8eeOQRyMiAW26xw1mvXt14QSulmj1NBE1MiH8In179Kf+T+j88s/gZrvrwKopKi45/QHQ0PPQQ7NoF995r33v1gu7dYe3axgtcKdVsaSJognx9fHn+4ud55oJnmL1+Nue8eQ67D+4+8UHh4fC3v9mxiQA2bIAePeD++2H3SY5VSnk1TQRN2F1n3MWHYz9k5Z6VDPzXQNJ21mKwvbPOsg+Ot22DTp1scmjb1j5QVkqpGmgiaOKuSLmCH67/AV/xZdhrw07coshVUhJs3Fg9/eWECTBjBpSXuy9YpVSzpImgGejbpi9Lb1zK4ITBTPjPBCZ/PpnisuKTH+jra4em2LjRrt98M/j52aRQOSOaUsrraSJoJuJD4/nyt19y+8DbeWbxMwx9dShZBVm1O7hLFygtrb499O678JvfQEgI/PWvWktQystpImhG/H39efaiZ5kzbg4bcjbQ84WevL/6/dod7OdneyOXl9smpxdfbMv/+Ee77bPP3Be4UqpJ00TQDF3S7RJ+vvlnTo8/nXH/HsdVH17F3sK9tTvYx8eOXfTJJ1BYCNOm2fKLLoLf/959QSulmixNBM1Ul9guLJy0kEdGPsKcDXPo8mwXnvrxKUrKS2p/kpAQuOceWLrUrj/9NFx2mc6IppSX0UTQjPn7+vPQ2Q+x/OblnNX+LO754h56vtCThVsXHn8azJqkptoHx2PHwuzZdka00aPtcwWlVIuniaAFSIlP4ZPxnzB3/Fxyi3IZ/vpwhrwyhG+3fFv7kwQF2ZZElc1NP/0UAgLgyy/dE7RSqsnQRNBCiAiju44m865Mnr/oeXYd3MWv3vwVk2ZPIn1feu1PdMUV9mFy3752/fzzYcoUbVmkVAsmdbqF0ASkpqaatLRa9LD1codKD/HHr//IjGUzqDAV3HvmvTx09kME+gXW/iSrVsGVV8KmTXbY6/POs4mhTx/3Ba6UcgsRWWaMSa1xmyaClm1HwQ4e+PoB3l71NokRidx75r3cNOAmgvyCancCY+Cqq6pvGQFccokdw2joUPcErZRqcCdKBG69NSQio0Rkg4iki8gDNWz/vYisFZFVIvK1iHRwZzzeKCEigbcue4svf9fMNRAAABavSURBVPslyVHJ3PX5XSQ+lcitn9zKlv1bTn4CEfjwQygutp3PAObOtWMaidjnCOPHw7/+ZW8pKaWaHbfVCETEF9gInAdkAUuB8caYtS77jAQWG2MOicitwAhjzG9OdF6tEZyab7Z8w4xlM5i9fjblppzr+l7Hn4b9iQ5RdcjBv/xim5q+9tqR5SJwww0weDBcd53ts6CUahI8cmtIRIYAU4wxFzjrDwIYY/56nP37Ac8ZY054v0ETQcPYUbCDx//7ODOWz8AYw297/5Z7zryHlPiUup9s/XpYswbeegs+/ri6/Oyz4Ztv7JhHSimP8tStoQRgu8t6llN2PL8DahznQERuEpE0EUnLzs5uwBC9V0JEAs9e9Czpd6Tzu36/442Vb9D7xd784Ys/sK9oX91O1r27bW00e7bte/C3v0FsLCxcaIev+O47nVNZqSasSdTdReQaIBV4sqbtxpgZxphUY0xqfHx84wbXwiVFJvHi6BfJuCuDCb0n8NSPT5HwVAJ/WfAXCksK635CPz87U1pODpx7ri0bMcLeJnriCU0ISjVB7kwEO4Akl/VEp+wIInIu8CfgUmPMYTfGo06gfWR73vj1Gyy+YTGnx53OlO+m0P6Z9vx90d8pOFxQv5N+8QWsXAkvvGCHs3jgAYiJscNabNrUsBeglKo3dz4j8MM+LD4HmwCWAlcbY9a47NMPmAWMMsbU6jeDPiNwP2MM3239jse+f4yvMr4i1D+UG/rfwDW9r2FA2wGISN1PWlgIL75oeywvWGDLLrzQPlTu3Bn692/Qa1BKHclj/QhE5CLgGcAXeNUY85iITAXSjDFzROQroBewyzlkmzHm0hOdUxNB4zHGsHDrQv657J/MWjuL0opS4kPiuTX1Vq7rdx3JUcn1O3FGBkycCD/+WN1j2c/P1hratLGd2P70J7uulGoQ2qFMnbL9Rfv5eMPH/Gfdf5i7cS7+Pv4MTx7OiA4jGJw4mGHth9Wt1zLYvgkffwwbNtg+CGvW2AfLlQYMgEmT4LbbbNNUpVS9aSJQDWrzvs08t+Q5vt7yNb/srR6yemzKWG5JvYURySPwkXo+fioshDfegHnzYPly2LXLzp8weLAd80hvISlVL5oIlNvsOrCLBZkLmPbjNDLzMtlXtI/2ke2Z0GsCV/e6mp6tetb/5GVl8MgjMHNm9bzLPXvCjTfCwIE2OWinNaVqRROBahRFpUV8tP4j3lr1Fl9u/pJyU84Vp1/B5adfzmXdLyPYP7j+J8/NtdNqvv02HDpkyzp0gGuugZEj7UB4sbF6C0mp49BEoBrd3sK9PPnDk7z888vkFecRHhDO2JSxjDptFKNOG0V4YHj9TlxebmsHCxfCq69CWhpUVNht3bvDuHF2op2ePW2iUEoBmgiUB1WYCr7L/I43Vr7Bh2s/5FDpIQJ9A7m468WM6TaGkckjSYpMOvmJjqegAH74AZYsgc8/h59+qt6WlGTHROrc2c66preRlBfTRKCahMNlh1myYwkfrv2QD9Z8wJ5CO1ppQngCQ5KGMKjdIAYnDmZQwqDaD5N9tEWLICsLpk2rnosZbA3hsstsQujY0U68o2MgKS+iiUA1OcYYft79Mz9s+4Efs37kx6wfyczLBCDAN4DBCYMZnDCYS7pdwplJZ+Ln41f3DykqgnXr4Ntv7W2k9eurbyN16AB5eXDaafYZwznn2EHytO+CaqE0EahmIedQDou2L2Lh1oX8d9t/+Xn3z5SUlxAZGMm5nc5l1GmjuKDzBfW/lVRUBGvX2mG0n3sOdu+2/Rd8fKCkxNYQ+vaFsWPtOElJSbbpqlItgCYC1SwdOHyALzO+5PP0z/ks/TOyCrIASIlPYVTnUYzsOJKz2p9FVFDUqX3QoUO2I9vs2fDmm7ajG9gWSCNH2r4LgwZBSop9acsk1QxpIlDNnjGGtdlr+Tz9c+Zvns+CzAWUVpQC0C22G6ntUjmr/Vmc1+k8OkV3qt94SPaDbA/nJUuqezr//HP1LaX27e3czQMG2ATRuzcEn0KzWKUaiSYC1eIUlRbxU9ZPLNq+iMU7FrN051J2H9wNQFxIHP3a9GN019EM7zCcbnHd6v/wGWD/ftiyxQ6W9/77sHp1dV8GH5/qB9EXXGBrDvoQWjVBmghUi2eMYWPuRr7e8jXLdi7jpx0/sTbbzorq7+PP0PZDGdhuIP3a9OP0+NPpEd8Df1//+n1YeTls325rCitWwDvvwObNdltsrH3GMHIkJCfbh9KtWuntJOVxmgiUV8rYn8GSHUtYvms5X2V8xZrsNZSUlwAQ6BtIp+hOdIvrRkpcCv3a9mN4h+HEh9Zj4qOKCttkddEi+PBD25+hssYAEBgIvXrZYbc7d7bNV+Pj7XvQKdRUlKoDTQRKASXlJazPWc/a7LUs27mMjfs2siFnA5v3b6asogyArrFd6d+2P/3a9KNXq14MSRpS94fRRUV2RNXt22HrVsjMhC+/tK2Vjv7/NnQo/PrXtuYwZAgkJGjtQbmFJgKlTuBw2WGW71rOwq0LWZS1iBW7V7Atf1vV9k7Rnege153usd3pEtuFPq370CW2C3EhcXX8oMOwbZtNDNu322EyPvnEjqNUKT7eTu159tlw+unQqZNtxupXj34USrnQRKBUHe0v2s/yXctZunMpaTvT2Ji7kfR96RSVFVXt0y68HV1ju9IzvidnJp1Jj1Y96Bbbre7zMmRnw5w5sG+ffRD97bc2UVTy9bWtlTp1sjWH2Fh7iykpyY6rpH0dVC1oIlCqARhjyNifwfqc9azLWceqPatI35fOqj2rKCwtBMBHfOgU3YmerXrSM74n3eNsLaJbbDcigyJr+0H2mUN6up3NbcsW+56RYW817d9vaxeVYmPtXNBt29qB95KSoEcPWxYUBHFxNonoLSevpolAKTcqKS9hQ84G1mSvYW32WtblrOOXPb+Qvi+dcmOn4hSEhIgEOkZ1pENUBzpEdqBHfA9S4lPoHNOZsICwOnxgia1FbN5s+ztkZFQ3cc3IsNuO1qaNHU7j9NOhdWuIiLD9IVJSICCggX4SqinTRKCUBxwuO8zm/ZvZlLuJX/b+wqZ9m8jYn8H2/O1kFWRVJQmA2OBYOkZ3JDkqmeTIZJKjkqvWO0R2IDQgtPYfXFhobzEVFtqaw9at8P33toaRmWkTReX/+4AA+4A6OtoOq9G/v739FBlpXyEh9l1rE82eJgKlmpjDZYfZkLuBddnryMzLZEveFjLzMqteh8sPH7F/fEj8CRNFnSb9KS+3rZpWrrTTge7YYed4+OUXW9s4WkiIfUYRHm4TRkyMTRbBwbZpbHS0TSZt29rOdYF1fEaiGoUmAqWakQpTwZ6De45IDK6JYmv+1qr+EJVah7Y+JlG0j2xPYkQiHaI6EBEYcfIPLiqCTZvsg+qCAvs6cAB27rS1isJCO2JrVpadS7py2A1X/v721lNUlE0Qru+uy76+9vO6d4czztDbU41AE4FSLUiFqWDXgV0nTBSV/SIqRQZG0j6yPW3D29IqtBXtwtrRIaoDiRGJVa+4kDh8pA6T91RU2FtPO3faFk+bN9ue1nv22ISRl2efXVQu5+fXfB5fX5sg+va1zzEiI+0D7vbt7Xu7draVlH89e4IrQBOBUl6lvKKcnQd2si1/G1kFWWzL38bW/K1sL9jO7oO72Vu4l50Hdh5Tq/D38ad1WGvahLUhITyBpIgkWoe1Jj4knvjQeOJD4qvWo4Ki6j6wX3m5rWXk5cHBg/aB9ZIl9hbVzp2weLF9fpGfX/MtqtBQezsqIMA2me3Spbr2ERVl+2D06AGJiXZZZ6Q7gscSgYiMAv4B+AIvG2MeP2r72cAzQG9gnDFm1snOqYlAqVNXXlHO3sK97Diwg6yCLLbnb2fngZ3sLtzNrgO72HFgB9vzt5N/uOa/4v18/IgLiSM+JN6+O4miVWgr2oW3o01YG2KCY4gLiSMuJI6ooKi61TZyc+18ETk51b2z8/Ls7aTS0uqe25WJ5ejfY76+Nkm0bWtrFG3b2tpFVJR9xlH5rMP1FRzcoh+KeyQRiIgvsBE4D8gClgLjjTFrXfZJBiKAPwBzNBEo1bQcLjtMzqEccg7lsLdwL7sP7ib7UDY5h3LILsyuXj6UTXZhNvuL99d4Hl/xJTo4mriQOFqFtiI2OJa4kDhig2NpHdaa2ODYqsTRKrQVMcExRARG1K7WUVFR/Sxj3Tr7/GLnTvte+aq8fVVefvzzBAbaRNG2re13ERMDYWH2YXlIiL01FR5u+2ZERVVPXBQf3yyGIj9RInBnv/VBQLoxJsMJYiYwBqhKBMaYTGdbDU+dlFKeFugXSEJEAgkRCbXav6S8hF0HdrGncA/7ivZVJZGcQznkHsolp8gmlI25G1m0fRG5RbnHPM+o5CM+RAVFER1kE0hsiE0e0UHRtA5tXVXbiA2JJSooiqh2UcR2voAQ/5CaE4gx9pbU/v02KVS+XNf377fDgKxbZ5cLC+2rpgfjrsLDbQ2ksvVUZU0kIcHeqoqNrX5QHhTU5Goe7kwECYBLP3mygMH1OZGI3ATcBNC+fftTj0wp5RYBvgG2w1xUh1rtb4xhf/F+9hXtY1/RPvYW7q1KHPnF+VXbcoty2X1wN6v3rmZf0T4Olhw87jkDfQOrkkZlTSMyMJKIwAiig6OJDoomOjiamKQYorskEB3cs6oswLeG1kvG2NtRpaW25lFUZG9d7doFe/fa1549dn3HDkhLszUQ1xFoj/ghBRzbosr13d/f1k4qax5RUfaYggI7pEiXLrX62dZFsxjJyhgzA5gB9taQh8NRSjUQESEmOIaY4Jg6HVdUWkRuUW5V0sgrziOvOM8mjUO2vHL7upx15Bfnk384/4QJBCDUP5SY4JiqhBERGFH1qkwmIf4hRAZFEt0lmujeXYgKGkhkYCRRQVGEB4bbZyHG2F/cO3bYZxmuLaiOfs/JsZ39KtdPdPvqxRebXSLYAbjOMp7olCml1CkJ9g8m0d82e62LsoqyqoSxv2g/+4v3s7/I1jqqlott7SSvOI/tBdspOFxAweEC8ovzq6ZHPR5BiAiMICooisggmxyigqIIDwgnLDGMsE5hhAWEERYQ77xXv0L9QwmrfBFAWIUfQXkHkUOH7DzaERH2dpMbuDMRLAW6iEhHbAIYB1ztxs9TSqkTqmztVOchxLG3sYrLiikqKyK/OL8qWeQV55F/OL96uTifvMN5Vbe2tuZt5WDJQQ6UHKCwpLBqgMLa8BGf6iQREMZfRvyF8b3G1zn2k3FbIjDGlInI7cB8bPPRV40xa0RkKpBmjJkjIgOBj4Bo4BIR+Ysxpoe7YlJKqfoSEYL9gwn2DyYmOIaO0R3rdZ7yinIKSws5WHKwKjEcLDl4xKuwpIay0sL6zaBXC9qhTCmlvMCJmo9q1zullPJymgiUUsrLaSJQSikvp4lAKaW8nCYCpZTycpoIlFLKy2kiUEopL6eJQCmlvFyz61AmItnA1noeHgfkNGA4zYFes3fQa/YOp3LNHYwxNXZNbnaJ4FSISNrxeta1VHrN3kGv2Tu465r11pBSSnk5TQRKKeXlvC0RzPB0AB6g1+wd9Jq9g1uu2aueESillDqWt9UIlFJKHUUTgVJKeTmvSQQiMkpENohIuog84Ol46ktEkkTkWxFZKyJrROQupzxGRL4UkU3Oe7RTLiIy3bnuVSLS3+VcE539N4nIRE9dU22JiK+I/CwinzjrHUVksXNt74tIgFMe6KynO9uTXc7xoFO+QUQu8MyV1I6IRInILBFZLyLrRGRIS/+eRWSy8+96tYi8JyJBLe17FpFXRWSviKx2KWuw71VEBojIL84x00VEThqUMabFv7BTZW4GOgEBwEogxdNx1fNa2gL9neVwYCOQAvwNeMApfwB4wlm+CPgMEOAMYLFTHgNkOO/RznK0p6/vJNf+e+Bd4BNn/QNgnLP8EnCrs/w/wEvO8jjgfWc5xfnuA4GOzr8JX09f1wmu9w3gBmc5AIhqyd8zkABsAYJdvt9JLe17Bs4G+gOrXcoa7HsFljj7inPshSeNydM/lEb6wQ8B5rusPwg86Om4GujaPgbOAzYAbZ2ytsAGZ/mfwHiX/Tc428cD/3QpP2K/pvYCEoGvgV8Bnzj/yHMAv6O/Y+w82UOcZT9nPzn6e3fdr6m9gEjnl6IcVd5iv2cnEWx3frn5Od/zBS3xewaSj0oEDfK9OtvWu5Qfsd/xXt5ya6jyH1ilLKesWXOqwv2AxUBrY8wuZ9NuoLWzfLxrb24/k2eA+4AKZz0WyDPGlDnrrvFXXZuzPd/Zvzldc0cgG3jNuR32soiE0oK/Z2PMDmAasA3Yhf3eltGyv+dKDfW9JjjLR5efkLckghZHRMKAfwN3G2MKXLcZ+6dAi2kXLCKjgb3GmGWejqUR+WFvH7xojOkHFGJvGVRpgd9zNDAGmwTbAaHAKI8G5QGe+F69JRHsAJJc1hOdsmZJRPyxSeAdY8x/nOI9ItLW2d4W2OuUH+/am9PPZChwqYhkAjOxt4f+AUSJiJ+zj2v8VdfmbI8Ecmle15wFZBljFjvrs7CJoSV/z+cCW4wx2caYUuA/2O++JX/PlRrqe93hLB9dfkLekgiWAl2c1gcB2AdLczwcU704LQBeAdYZY55y2TQHqGw5MBH77KCy/Fqn9cEZQL5TBZ0PnC8i0c5fYuc7ZU2OMeZBY0yiMSYZ+919Y4yZAHwLXOnsdvQ1V/4srnT2N075OKe1SUegC/bBWpNjjNkNbBeRbk7ROcBaWvD3jL0ldIaIhDj/ziuvucV+zy4a5Ht1thWIyBnOz/Bal3Mdn6cfmjTiw5mLsC1sNgN/8nQ8p3AdZ2GrjauAFc7rIuy90a+BTcBXQIyzvwDPO9f9C5Dqcq7rgXTndZ2nr62W1z+C6lZDnbD/wdOBD4FApzzIWU93tndyOf5Pzs9iA7VoTeHha+0LpDnf9Wxs65AW/T0DfwHWA6uBt7Atf1rU9wy8h30GUoqt+f2uIb9XINX5+W0GnuOoBgc1vXSICaWU8nLecmtIKaXUcWgiUEopL6eJQCmlvJwmAqWU8nKaCJRSystpIlBeS0QOOu/JInJ1A5/7j0etL2rI8yvVkDQRKGUHAKtTInDp6Xo8RyQCY8yZdYxJqUajiUApeBwYJiIrnPHwfUXkSRFZ6owBfzOAiIwQke9FZA62xysiMltEljlj6N/klD0OBDvne8cpq6x9iHPu1c6Y8b9xOfcCqZ5/4J1ajSOvVAM42V81SnmDB4A/GGNGAzi/0PONMQNFJBD4QUS+cPbtD/Q0xmxx1q83xuwTkWBgqYj82xjzgIjcbozpW8NnXY7tMdwHiHOOWehs6wf0AHYCP2DH2flvw1+uUkfSGoFSxzofO77LCuwQ37HY8WoAlrgkAYA7RWQl8BN2ELAunNhZwHvGmHJjzB7gO2Cgy7mzjDEV2KFDkhvkapQ6Ca0RKHUsAe4wxhwxOJuIjMAOB+26fi520pNDIrIAO/5NfR12WS5H/3+qRqI1AqXgAHbaz0rzgVud4b4Rka7OpDBHiwT2O0mgO3Z6wEqllccf5XvgN85ziHjstIVNfWRM1cLpXxxK2dE9y51bPK9j5zpIBpY7D2yzgV/XcNznwC0isg47yuVPLttmAKtEZLmxQ2ZX+gg73eJK7Ciy9xljdjuJRCmP0NFHlVLKy+mtIaWU8nKaCJRSystpIlBKKS+niUAppbycJgKllPJymgiUUsrLaSJQSikv9/+KkaWMfh7g9AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa9AxYh_cOMD",
        "colab_type": "text"
      },
      "source": [
        "## Sklearn Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oDRuzR-cRRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQhMTMPxcTy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f2e39074-ce84-4321-f8c6-3b3de278783a"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "x, y = preprocessor.pre_process(2)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=4)\n",
        "logistic_regression = LogisticRegression()\n",
        "logistic_regression.fit(x_train,y_train)\n",
        "y_pred = logistic_regression.predict(x_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Test:\",accuracy)\n",
        "y_pred1 = logistic_regression.predict(x_train)\n",
        "accuracy = metrics.accuracy_score(y_train, y_pred1)\n",
        "print(\"Accuracy Train:\",accuracy)\n",
        "print(\"Thetas:\",logistic_regression.coef_)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Test: 0.9795918367346939\n",
            "Accuracy Train: 0.9825072886297376\n",
            "Thetas: [[-4.58128443 -4.68888037 -4.27457391  0.23536925]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOUVjO6MSZBi",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdbZwPiFGwiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3f8b307d-02e5-4467-8623-aa233a182bba"
      },
      "source": [
        "Xtrain = np.array([[1, 2, 3], \n",
        "                   [4, 5, 6]])\n",
        "ytrain = np.array([1, 2])\n",
        "\n",
        "Xtest = np.array([[7, 8, 9]])\n",
        "ytest = np.array([3])\n",
        "\n",
        "print('Linear Regression')\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(Xtrain, ytrain)\n",
        "\n",
        "ypred = linear.predict(Xtest)\n",
        "\n",
        "print('Predicted Values:', ypred)\n",
        "print('True Values:', ytest)\n",
        "\n",
        "print('Logistic Regression')\n",
        "\n",
        "logistic = MyLogisticRegression()\n",
        "logistic.fit(Xtrain, ytrain)\n",
        "\n",
        "ypred = logistic.predict(Xtest)\n",
        "\n",
        "print('Predicted Values:', ypred)\n",
        "print('True Values:', ytest)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Predicted Values: [3.47084775]\n",
            "True Values: [3]\n",
            "Logistic Regression\n",
            "Predicted Values: [1.]\n",
            "True Values: [3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}