{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLVQXaHvi+a+wOG69u8ZRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/ML_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5O3iEtAFqzd",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEo5gx2hZQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9RJnOQAEAU3",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kS_U4I6EDez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyPreProcessor():\n",
        "    \"\"\"\n",
        "    My steps for pre-processing for the three datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def pre_process(self, dataset):\n",
        "        \"\"\"\n",
        "        Reading the file and preprocessing the input and output.\n",
        "        Note that you will encode any string value and/or remove empty entries in this function only.\n",
        "        Further any pre processing steps have to be performed in this function too. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        dataset : integer with acceptable values 0, 1, or 2\n",
        "        0 -> Abalone Dataset\n",
        "        1 -> VideoGame Dataset\n",
        "        2 -> BankNote Authentication Dataset\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "        y : 1-dimensional numpy array of shape (n_samples,)\n",
        "        \"\"\"     \n",
        "\n",
        "        if dataset == 0:\n",
        "            df=pd.read_csv('/content/Dataset.data',delim_whitespace=True,header=None) # data read from file\n",
        "            df.sample(frac=1) # data shuffled\n",
        "\n",
        "            # changed gender values to integers\n",
        "            df[0].replace('M',1,inplace=True)\n",
        "            df[0].replace('F',2,inplace=True)\n",
        "            df[0].replace('I',3,inplace=True)\n",
        "\n",
        "            data=df.to_numpy() # converted dataframe into numpy array\n",
        "            X=data[:,:-1]\n",
        "            y=data[:,-1]\n",
        "\n",
        "        elif dataset == 1:\n",
        "            df=pd.read_csv('/content/VideoGameDataset.csv') # data read from file\n",
        "            df=df[['Critic_Score','Global_Sales','User_Score']] # required colums extracted\n",
        "            df=df.sample(frac=1) # data shuffled\n",
        "\n",
        "            df['Critic_Score'].fillna(df['Critic_Score'].mean(), inplace=True) # replaced NaN values with median of the column\n",
        "            df['User_Score'].replace(to_replace = 'tbd', value = np.nan,inplace=True) # replaced the cell with 'tbd' value to NaN value in the colum\n",
        "            df['User_Score']=df['User_Score'].astype(np.float) # converted column from strings to float values\n",
        "            df['User_Score'].fillna(df['User_Score'].mean(), inplace=True) # replaced NaN values with median of the column\n",
        "\n",
        "            data=df.to_numpy() # converted dataframe into numpy array\n",
        "            X=data[:,:-1]\n",
        "            y=data[:,-1]\n",
        "\n",
        "        elif dataset == 2:\n",
        "            # Implement for the banknote authentication dataset\n",
        "            df=pd.read_csv('/content/data_banknote_authentication.txt',header=None) # data read from file\n",
        "            df=df.sample(frac=1) # data shuffled\n",
        "            X=df[[0,1,2,3]].to_numpy()\n",
        "            y=df[[4]].to_numpy()\n",
        "            y=y.squeeze()\n",
        "\n",
        "        X=(X-X.mean(axis=0))/X.std(axis=0) # normalized the data \n",
        "\n",
        "        return X, y\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSTeIMNDSrEl",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofY4sQaGeYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLinearRegression():\n",
        "  \"\"\"\n",
        "\tMy implementation of Linear Regression.\n",
        "\t\"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def cross_validation(self,X,y,epoch=1000,alpha=0.01,k=10,lossfunc=1):\n",
        "    \"\"\"\n",
        "    performs k fold cross validation on the given dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) \n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    k : Number of folds the data needs to be splitted into\n",
        "\n",
        "    epoch : Number of times gradient descent has to run\n",
        "\n",
        "    alpha : Learning rate of gradient descent\n",
        "\n",
        "    lossfunc: determines which loss function to call\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=X.shape[0] # number of samples\n",
        "\n",
        "    split_start=0 # initial split's first index\n",
        "    split_end=m//k # initial split's last index\n",
        "\n",
        "    theta_list=[0]*k  # initialized theta\n",
        "    training_loss_list=[0]*k  # initialized list to store all the training loss from every fold\n",
        "    validation_loss_list=[0]*k # initialized list to store all the validation loss from every fold\n",
        "\n",
        "    error_min=float(\"inf\")\n",
        "    idx=0\n",
        "\n",
        "    for i in range(k):\n",
        "\n",
        "      # Extracting X and y for train and test set\n",
        "      X_train=np.concatenate((X[:split_start],X[split_end:]),axis=0)\n",
        "      y_train=np.concatenate((y[:split_start],y[split_end:]),axis=0)\n",
        "\n",
        "      X_test=X[split_start:split_end]\n",
        "      y_test=y[split_start:split_end]\n",
        "\n",
        "      self.fit(X_train,y_train,X_test,y_test,epoch,alpha,lossfunc) # calculating model parameters by running the gradient descent\n",
        "\n",
        "      # storing the results of current fold in the array\n",
        "      theta_list[i]=self.theta\n",
        "      training_loss_list[i]=self.training_loss\n",
        "      validation_loss_list[i]=self.validation_loss\n",
        "\n",
        "      split_start=split_end # updating slice parameters\n",
        "      split_end+=m//k\n",
        "\n",
        "      error=training_loss_list[i][-1]\n",
        "\n",
        "\n",
        "      # if the error in this fold is minimum of all errors seen upto now then update it and store the fold number\n",
        "      if(error<error_min):\n",
        "        idx=i\n",
        "        error_min=error\n",
        "    \n",
        "    # final storing the values associated with minimum error \n",
        "    self.theta=theta_list[idx]\n",
        "    self.training_loss=training_loss_list[idx]\n",
        "    self.validation_loss=validation_loss_list[idx]\n",
        "\n",
        "  def MSE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Mean Squared Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "\n",
        "    X_trans=np.transpose(X)                                 # Transpose of vector X\n",
        "    err=X.dot(theta)-y\n",
        "    derv =(1/m)*(X_trans.dot(err))                         # Calculates X` * ( X*theta - y )\n",
        "    \n",
        "    return derv,sum(err**2)/m    \n",
        " \n",
        "  def MAE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Mean Absolute Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "    err=(1/m)*(X.dot(theta)-y)          # Calculates (1/m) *( X*theta - y )\n",
        "    X_trans=X.T                         # Transpose of vector X\n",
        "    epsilon=10**-7\n",
        "    derv=(1/m)*(X_trans.dot(abs(err)/(err+epsilon)))\n",
        "\n",
        "    return derv,np.sum(abs(err))           # returns gradient and sum((1/m) *( |X*theta - y |))\n",
        "\n",
        "  def RMSE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Root Mean Squared Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) \n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "    X_trans=X.T                                          # Transpose of vector X\n",
        "    diff=X.dot(theta)-y                                  # Calculates ( X*theta - y )\n",
        "   \n",
        "    err=((1/m)*np.sum((diff)**2))**0.5                   # Calculates (1/m) *sqrt(sum((X*theta - y)^2))\n",
        "    derv =(1/m)*(X_trans.dot(diff))/err         \n",
        "    \n",
        "    return derv,err\n",
        "\n",
        "\n",
        "  def gradient_descent(self,X,y,X_test,y_test,epochs,alpha,lossfunc):\n",
        "    \"\"\"\n",
        "    Finding theta using the gradient descent method\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    X_test : 2-dimensional numpy array of shape (n_samples, n_features) which acts as Testing data.\n",
        "\n",
        "    y_test : 1-dimensional numpy array of shape (n_samples,) which acts as Testing labels.\n",
        "\n",
        "    epochs : Number of times gradient descent has to run\n",
        "\n",
        "    alpha : Learning rate of gradient descent\n",
        "\n",
        "    lossfunc: determines which loss function to call\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\n",
        "    training_loss: Calculated training loss at every theta\n",
        "\n",
        "    tvalidation_loss: Calculated validation loss at every theta\n",
        "    \"\"\"\n",
        "   \n",
        "    theta= np.zeros((X.shape[1],))                      # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "    \n",
        "    training_loss=np.array([])  # initializing array to store training loss at every value of theta\n",
        "    validation_loss= np.array([]) # initializing array to store validation loss at every value of theta\n",
        "\n",
        "    print(\"-*\"*20)\n",
        "\n",
        "    for i in range(epochs): \n",
        "      if(lossfunc==1): \n",
        "        derv,train_loss=self.RMSE(X,y,theta)   \n",
        "      elif(lossfunc==2):   \n",
        "        derv,train_loss=self.MAE(X,y,theta)\n",
        "      else:\n",
        "        derv,train_loss=self.MSE(X,y,theta)\n",
        "      training_loss=np.append(training_loss,train_loss)\n",
        "\n",
        "      \n",
        "\n",
        "      if(X_test is not None): # calculate validation loss only if test set is provided\n",
        "        if(lossfunc==1):\n",
        "          derv_val,val_loss=self.RMSE(X_test,y_test,theta)   \n",
        "        elif(lossfunc==2):   \n",
        "          derv_val,val_loss=self.MAE(X_test,y_test,theta)\n",
        "        else:\n",
        "          derv_val,val_loss=self.MSE(X_test,y_test,theta)\n",
        "      \n",
        "        validation_loss=np.append(validation_loss,val_loss)\n",
        "        if(i%400==0):\n",
        "          print(\"Iteration:\",i,\"Training error:\",train_loss,\"Validation error:\",val_loss)\n",
        "      theta=theta-alpha*derv      \n",
        "  \n",
        "    return theta,training_loss,validation_loss\n",
        "\n",
        "  def fit(self, X, y,X_test=None,y_test=None,epoch=400,alpha=0.01,lossfunc=1):\n",
        "    \"\"\"\n",
        "    Fitting (training) the linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self : an instance of self\n",
        "    \"\"\"\n",
        "\n",
        "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1) # Adding a bias variable i.e columns of 1 to data\n",
        "\n",
        "    if(X_test is not None): # if validation set is provided then add a bias variable i.e columns of 1 to data\n",
        "      X_test=np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis=1)\n",
        "   \n",
        "    X_trans=np.transpose(X)\n",
        "    if(lossfunc==4):\n",
        "      try:\n",
        "        self.theta = np.linalg.inv(X_trans.dot(X)).dot(X_trans).dot(y)  # using the normal eqn, theta = inv(X`*X)*X`*y\n",
        "      except:\n",
        "        self.theta,self.training_loss,self.validation_loss = self.gradient_descent(X,y,X_test,y_test,epoch,alpha,lossfunc=1) # using the gradient descent method with RMSE loss function if the given data is non invertible\n",
        "    else: \n",
        "      self.theta,self.training_loss,self.validation_loss = self.gradient_descent(X,y,X_test,y_test,epoch,alpha,lossfunc) # using the gradient descent method with given number of epochs and learning rate\n",
        "      \n",
        "\n",
        "    # fit function has to return an instance of itself or else it won't work with test.py\n",
        "    return self\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predicting values using the trained linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "    \"\"\"\n",
        "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
        "    y=np.dot(X,self.theta)\n",
        "    # return the numpy array y which contains the predicted values\n",
        "    return y\n",
        "\n",
        "  def plot_loss(self):\n",
        "    print(\"Thetas:\",self.theta)\n",
        "    print(\"Training Loss:\",self.training_loss[-1])\n",
        "    print(\"Validation Loss:\",self.validation_loss[-1])\n",
        "    \n",
        "    x=np.arange(self.training_loss.shape[0])\n",
        "    plt.plot(x,self.training_loss,color=\"g\", label=\"Training Loss\")\n",
        "    plt.plot(x,self.validation_loss,color=\"r\",label=\"Validation Loss\")\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68z99ULqRBt",
        "colab_type": "text"
      },
      "source": [
        "## Dataset1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrZ20anXvGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(0)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(X,y,lossfunc=4)\n",
        "print(linear.theta)\n",
        "linear.cross_validation(X,y,epoch=10000,alpha=0.01,lossfunc=2)\n",
        "linear.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHOIS1Wb9tpj"
      },
      "source": [
        "## Dataset2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpSzdHFk9tpz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d72938d-64c9-4319-b636-90c33371e2f6"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(1)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(X,y,lossfunc=4)\n",
        "print(linear.theta)\n",
        "linear.cross_validation(X,y, epoch=10000,alpha=0.01, lossfunc=2)\n",
        "linear.plot_loss()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 7.12504611  0.51072865 -0.02992483]\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.127312896491453 Validation error: 7.104632855760806\n",
            "Iteration: 400 Training error: 3.2052905849244073 Validation error: 3.192771356822162\n",
            "Iteration: 800 Training error: 0.45950048349692996 Validation error: 0.48303026134665206\n",
            "Iteration: 1200 Training error: 0.4586564816514576 Validation error: 0.48322748740898347\n",
            "Iteration: 1600 Training error: 0.45838211356644226 Validation error: 0.4830750183851089\n",
            "Iteration: 2000 Training error: 0.4583709932394523 Validation error: 0.48287665036305083\n",
            "Iteration: 2400 Training error: 0.45868593215449316 Validation error: 0.48334258268308267\n",
            "Iteration: 2800 Training error: 0.45836624158977407 Validation error: 0.48306262827078345\n",
            "Iteration: 3200 Training error: 0.45867781978227434 Validation error: 0.4833321115908642\n",
            "Iteration: 3600 Training error: 0.45851438666175415 Validation error: 0.4832006179298973\n",
            "Iteration: 4000 Training error: 0.45868191432142125 Validation error: 0.4833073155558075\n",
            "Iteration: 4400 Training error: 0.4584325764976156 Validation error: 0.48312885571551323\n",
            "Iteration: 4800 Training error: 0.4587810928985936 Validation error: 0.4834719471201455\n",
            "Iteration: 5200 Training error: 0.45841428706124865 Validation error: 0.4829965719620446\n",
            "Iteration: 5600 Training error: 0.466389780455225 Validation error: 0.49105165214685176\n",
            "Iteration: 6000 Training error: 0.4586744125052083 Validation error: 0.48337841388074654\n",
            "Iteration: 6400 Training error: 0.45867265124116724 Validation error: 0.4834092682251416\n",
            "Iteration: 6800 Training error: 0.4586855993336322 Validation error: 0.48340000543084355\n",
            "Iteration: 7200 Training error: 0.4591934371777106 Validation error: 0.4836147236708976\n",
            "Iteration: 7600 Training error: 0.45868338346610227 Validation error: 0.48331945069477394\n",
            "Iteration: 8000 Training error: 0.4716879344635603 Validation error: 0.49645192843335667\n",
            "Iteration: 8400 Training error: 0.4586684734668601 Validation error: 0.4833367731911258\n",
            "Iteration: 8800 Training error: 0.4672596602888355 Validation error: 0.4923880212137881\n",
            "Iteration: 9200 Training error: 0.625535830686875 Validation error: 0.6457705442694766\n",
            "Iteration: 9600 Training error: 0.4585062241962061 Validation error: 0.48317534671848583\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.127244716774584 Validation error: 7.1052468404283395\n",
            "Iteration: 400 Training error: 3.2086775629023667 Validation error: 3.188947420299327\n",
            "Iteration: 800 Training error: 0.4622537264184415 Validation error: 0.4642513094326176\n",
            "Iteration: 1200 Training error: 0.4607213691196722 Validation error: 0.46205154398022186\n",
            "Iteration: 1600 Training error: 0.4606989384526372 Validation error: 0.4619535772202591\n",
            "Iteration: 2000 Training error: 0.4608703734517008 Validation error: 0.4621574826957535\n",
            "Iteration: 2400 Training error: 0.46069974323432095 Validation error: 0.46201822255751135\n",
            "Iteration: 2800 Training error: 0.46099083118694056 Validation error: 0.4622937806653512\n",
            "Iteration: 3200 Training error: 0.4609151235009834 Validation error: 0.46221001020340713\n",
            "Iteration: 3600 Training error: 0.46068922930915146 Validation error: 0.46196665544843785\n",
            "Iteration: 4000 Training error: 0.4607392695493267 Validation error: 0.4620208384024475\n",
            "Iteration: 4400 Training error: 0.460725281237527 Validation error: 0.46206937035135504\n",
            "Iteration: 4800 Training error: 0.4610409666687454 Validation error: 0.46234177773116636\n",
            "Iteration: 5200 Training error: 0.46099381665939304 Validation error: 0.46226631322969003\n",
            "Iteration: 5600 Training error: 0.46160629907196676 Validation error: 0.4627748163301077\n",
            "Iteration: 6000 Training error: 0.4607031110797182 Validation error: 0.46196590685464156\n",
            "Iteration: 6400 Training error: 0.46076475176663917 Validation error: 0.4620468576223927\n",
            "Iteration: 6800 Training error: 0.46091675578669405 Validation error: 0.46222141581393683\n",
            "Iteration: 7200 Training error: 0.460739918134106 Validation error: 0.46202709591181723\n",
            "Iteration: 7600 Training error: 0.4609399232805733 Validation error: 0.46223044866455953\n",
            "Iteration: 8000 Training error: 0.46099439448639223 Validation error: 0.4622661523187973\n",
            "Iteration: 8400 Training error: 0.46068478592368267 Validation error: 0.46197544673434965\n",
            "Iteration: 8800 Training error: 0.4607553007508203 Validation error: 0.4621143778621576\n",
            "Iteration: 9200 Training error: 0.4609863806310613 Validation error: 0.46228803658085843\n",
            "Iteration: 9600 Training error: 0.461028576878321 Validation error: 0.46232478168644225\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.125229471225689 Validation error: 7.123394904473679\n",
            "Iteration: 400 Training error: 3.2069817988538625 Validation error: 3.206588622430111\n",
            "Iteration: 800 Training error: 0.46269717935704346 Validation error: 0.4572890329098226\n",
            "Iteration: 1200 Training error: 0.6850824496068407 Validation error: 0.6948068642043352\n",
            "Iteration: 1600 Training error: 0.46139269670218863 Validation error: 0.45856429922979547\n",
            "Iteration: 2000 Training error: 0.4613860656726863 Validation error: 0.45878347844553485\n",
            "Iteration: 2400 Training error: 0.46114597965548787 Validation error: 0.4583970838766709\n",
            "Iteration: 2800 Training error: 0.46128841619439187 Validation error: 0.4585212271360718\n",
            "Iteration: 3200 Training error: 0.4621705634377831 Validation error: 0.4604507296026986\n",
            "Iteration: 3600 Training error: 0.46123474012027305 Validation error: 0.458537470131637\n",
            "Iteration: 4000 Training error: 0.46122060303393975 Validation error: 0.4585237993906228\n",
            "Iteration: 4400 Training error: 0.46114874156600255 Validation error: 0.4584133285935613\n",
            "Iteration: 4800 Training error: 0.4697555995780183 Validation error: 0.4675769502714099\n",
            "Iteration: 5200 Training error: 0.4613926525384998 Validation error: 0.45856390446617706\n",
            "Iteration: 5600 Training error: 0.46111974658359084 Validation error: 0.4584585510158359\n",
            "Iteration: 6000 Training error: 0.4613043552280878 Validation error: 0.45816695635323734\n",
            "Iteration: 6400 Training error: 0.4613659845828379 Validation error: 0.4584993589452182\n",
            "Iteration: 6800 Training error: 0.46130982929070163 Validation error: 0.45855413659155353\n",
            "Iteration: 7200 Training error: 0.4611770826210845 Validation error: 0.4582175666004513\n",
            "Iteration: 7600 Training error: 0.4613884278044556 Validation error: 0.45857256097432675\n",
            "Iteration: 8000 Training error: 0.4612619474507367 Validation error: 0.4584982242591557\n",
            "Iteration: 8400 Training error: 0.46114497433303125 Validation error: 0.45840144882599043\n",
            "Iteration: 8800 Training error: 0.46137988666107455 Validation error: 0.4586365698979362\n",
            "Iteration: 9200 Training error: 0.46118864355833444 Validation error: 0.45848297004767646\n",
            "Iteration: 9600 Training error: 0.46115132114382346 Validation error: 0.4584199532008323\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.124852470338235 Validation error: 7.126789942986194\n",
            "Iteration: 400 Training error: 3.207451974504156 Validation error: 3.2038458962652423\n",
            "Iteration: 800 Training error: 0.4634010165330654 Validation error: 0.4661779249638788\n",
            "Iteration: 1200 Training error: 0.4608724224402273 Validation error: 0.4630415630935102\n",
            "Iteration: 1600 Training error: 0.46094240152522586 Validation error: 0.46299409688370413\n",
            "Iteration: 2000 Training error: 0.4606123273483348 Validation error: 0.4626532896584844\n",
            "Iteration: 2400 Training error: 0.46097023667854164 Validation error: 0.46295849916688653\n",
            "Iteration: 2800 Training error: 0.461087424030617 Validation error: 0.4634223521918721\n",
            "Iteration: 3200 Training error: 0.4607502019885275 Validation error: 0.46280364699933196\n",
            "Iteration: 3600 Training error: 0.4607307949246246 Validation error: 0.4627771674915182\n",
            "Iteration: 4000 Training error: 0.46089175154291695 Validation error: 0.4628871803250555\n",
            "Iteration: 4400 Training error: 0.4606031842512274 Validation error: 0.46264060498538373\n",
            "Iteration: 4800 Training error: 0.4609010519324514 Validation error: 0.46294018107159396\n",
            "Iteration: 5200 Training error: 0.46060538036628196 Validation error: 0.462673303791143\n",
            "Iteration: 5600 Training error: 0.5057584991983971 Validation error: 0.506270155326555\n",
            "Iteration: 6000 Training error: 0.46090034320697937 Validation error: 0.46294989861419594\n",
            "Iteration: 6400 Training error: 0.4609459737620777 Validation error: 0.4629706141437314\n",
            "Iteration: 6800 Training error: 0.4609476184764123 Validation error: 0.4629917591873143\n",
            "Iteration: 7200 Training error: 0.46060211829703285 Validation error: 0.46269414324117186\n",
            "Iteration: 7600 Training error: 0.4609282160484064 Validation error: 0.46297246904905215\n",
            "Iteration: 8000 Training error: 0.4609459398649481 Validation error: 0.46297059572719557\n",
            "Iteration: 8400 Training error: 0.4608807381932123 Validation error: 0.46292142459899666\n",
            "Iteration: 8800 Training error: 0.46094457370878406 Validation error: 0.4629667648186692\n",
            "Iteration: 9200 Training error: 0.46094817685584044 Validation error: 0.4629812611065343\n",
            "Iteration: 9600 Training error: 0.4609394816928615 Validation error: 0.46299005930808707\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.124995266793181 Validation error: 7.125504005790492\n",
            "Iteration: 400 Training error: 3.204361700779434 Validation error: 3.2092535758533502\n",
            "Iteration: 800 Training error: 0.4608238755085318 Validation error: 0.4753589558786533\n",
            "Iteration: 1200 Training error: 0.4592955433360933 Validation error: 0.47442363376428714\n",
            "Iteration: 1600 Training error: 0.45946261670100397 Validation error: 0.47477819607026556\n",
            "Iteration: 2000 Training error: 0.4593082323661774 Validation error: 0.4745534851395869\n",
            "Iteration: 2400 Training error: 0.4595828480811428 Validation error: 0.4746615669096116\n",
            "Iteration: 2800 Training error: 0.45931556964034126 Validation error: 0.47450544982307574\n",
            "Iteration: 3200 Training error: 0.45929204293903986 Validation error: 0.474481758154295\n",
            "Iteration: 3600 Training error: 0.4596141179591058 Validation error: 0.4748062089768856\n",
            "Iteration: 4000 Training error: 2.7428120928286517 Validation error: 2.729088230209171\n",
            "Iteration: 4400 Training error: 0.4962708736652181 Validation error: 0.5141813366438583\n",
            "Iteration: 4800 Training error: 0.45948377069950497 Validation error: 0.474775354716788\n",
            "Iteration: 5200 Training error: 0.5840838023917798 Validation error: 0.5920490933183744\n",
            "Iteration: 5600 Training error: 0.459304380841684 Validation error: 0.4744688866145782\n",
            "Iteration: 6000 Training error: 0.4592933472125404 Validation error: 0.47449239421215106\n",
            "Iteration: 6400 Training error: 0.4595688856185986 Validation error: 0.47464567597823526\n",
            "Iteration: 6800 Training error: 0.4595806049132699 Validation error: 0.47474174613300435\n",
            "Iteration: 7200 Training error: 0.4593904856282221 Validation error: 0.4745508096248259\n",
            "Iteration: 7600 Training error: 0.45929195712619864 Validation error: 0.4744843700850596\n",
            "Iteration: 8000 Training error: 0.459328544213038 Validation error: 0.47456193950063436\n",
            "Iteration: 8400 Training error: 0.4592986790098579 Validation error: 0.4745058396100287\n",
            "Iteration: 8800 Training error: 0.4592978498927719 Validation error: 0.474493887840106\n",
            "Iteration: 9200 Training error: 0.46041027447984684 Validation error: 0.4758431537888661\n",
            "Iteration: 9600 Training error: 0.4593345719122178 Validation error: 0.47461033081851856\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.129750094383976 Validation error: 7.082684947988997\n",
            "Iteration: 400 Training error: 3.2093429120483656 Validation error: 3.169012208798749\n",
            "Iteration: 800 Training error: 0.5630121234842371 Validation error: 0.5868314317514287\n",
            "Iteration: 1200 Training error: 0.4580645304194495 Validation error: 0.48975346770414474\n",
            "Iteration: 1600 Training error: 0.4576329963013943 Validation error: 0.4894799988682286\n",
            "Iteration: 2000 Training error: 0.4576424240258946 Validation error: 0.48950625890668187\n",
            "Iteration: 2400 Training error: 0.4579525954677566 Validation error: 0.4898185529452738\n",
            "Iteration: 2800 Training error: 0.45764016482558106 Validation error: 0.4895053909776477\n",
            "Iteration: 3200 Training error: 0.4578131825483778 Validation error: 0.4896451293913974\n",
            "Iteration: 3600 Training error: 0.45797599049198134 Validation error: 0.48982013650170786\n",
            "Iteration: 4000 Training error: 0.4579375222048856 Validation error: 0.4897890505443102\n",
            "Iteration: 4400 Training error: 0.4579591426326544 Validation error: 0.48981475699018406\n",
            "Iteration: 4800 Training error: 0.4579390974597256 Validation error: 0.489811768364871\n",
            "Iteration: 5200 Training error: 0.4580380042758183 Validation error: 0.48971408597013844\n",
            "Iteration: 5600 Training error: 0.457643903752179 Validation error: 0.48949402666580033\n",
            "Iteration: 6000 Training error: 0.4576396940758204 Validation error: 0.48949140341057784\n",
            "Iteration: 6400 Training error: 0.4576410934636558 Validation error: 0.48950392089735306\n",
            "Iteration: 6800 Training error: 0.4578922569775763 Validation error: 0.48970198865507153\n",
            "Iteration: 7200 Training error: 0.45763210024537104 Validation error: 0.4894729581322415\n",
            "Iteration: 7600 Training error: 0.45795487768535215 Validation error: 0.4898185848631825\n",
            "Iteration: 8000 Training error: 0.45767320148737783 Validation error: 0.489506916983444\n",
            "Iteration: 8400 Training error: 0.4576398164770745 Validation error: 0.4895009583136224\n",
            "Iteration: 8800 Training error: 0.4578468081907751 Validation error: 0.4897366736124712\n",
            "Iteration: 9200 Training error: 0.4579362660228317 Validation error: 0.48980973044482884\n",
            "Iteration: 9600 Training error: 0.45772207313297386 Validation error: 0.4895962812923616\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.126056793053635 Validation error: 7.115944552069772\n",
            "Iteration: 400 Training error: 3.207149055658979 Validation error: 3.2028133047060896\n",
            "Iteration: 800 Training error: 0.46639683606790966 Validation error: 0.4347833838065528\n",
            "Iteration: 1200 Training error: 0.46407447305088134 Validation error: 0.4318025562405735\n",
            "Iteration: 1600 Training error: 0.4640648514135544 Validation error: 0.43171740677077636\n",
            "Iteration: 2000 Training error: 0.4640734443675228 Validation error: 0.43173164276850967\n",
            "Iteration: 2400 Training error: 0.46421783673690736 Validation error: 0.43198809403875793\n",
            "Iteration: 2800 Training error: 0.4640947753481851 Validation error: 0.4317146946362306\n",
            "Iteration: 3200 Training error: 0.4643663402569592 Validation error: 0.4320194096721005\n",
            "Iteration: 3600 Training error: 0.46404755102087947 Validation error: 0.43167276377099895\n",
            "Iteration: 4000 Training error: 0.5546207186986857 Validation error: 0.5296292771701535\n",
            "Iteration: 4400 Training error: 0.4640611915164879 Validation error: 0.43173349168095354\n",
            "Iteration: 4800 Training error: 0.46408350021099115 Validation error: 0.43170457299749887\n",
            "Iteration: 5200 Training error: 0.4640609267705763 Validation error: 0.43172034772071066\n",
            "Iteration: 5600 Training error: 0.46413023912660545 Validation error: 0.43183188511341675\n",
            "Iteration: 6000 Training error: 0.4698353387725112 Validation error: 0.43696837716349624\n",
            "Iteration: 6400 Training error: 0.46407433165845813 Validation error: 0.43166113133355655\n",
            "Iteration: 6800 Training error: 0.46408168374853886 Validation error: 0.43180364685502776\n",
            "Iteration: 7200 Training error: 0.4643385502647315 Validation error: 0.43201932386607506\n",
            "Iteration: 7600 Training error: 0.4643451363157182 Validation error: 0.4319607348553577\n",
            "Iteration: 8000 Training error: 0.4640952211754362 Validation error: 0.43172204701587424\n",
            "Iteration: 8400 Training error: 0.4642650546032207 Validation error: 0.4319081597441331\n",
            "Iteration: 8800 Training error: 0.46431048403492603 Validation error: 0.4318439412733795\n",
            "Iteration: 9200 Training error: 0.4643147559077069 Validation error: 0.4319762494008397\n",
            "Iteration: 9600 Training error: 0.464339097952523 Validation error: 0.43202439747779486\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.127228097142299 Validation error: 7.105396506632177\n",
            "Iteration: 400 Training error: 3.2064196522514874 Validation error: 3.1867356475863033\n",
            "Iteration: 800 Training error: 0.46201194535118534 Validation error: 0.46534136087157935\n",
            "Iteration: 1200 Training error: 0.46050666105216803 Validation error: 0.46358475035513386\n",
            "Iteration: 1600 Training error: 0.4605278907319818 Validation error: 0.4635742961162369\n",
            "Iteration: 2000 Training error: 0.46051669344684076 Validation error: 0.46356208942989097\n",
            "Iteration: 2400 Training error: 0.4605151021151296 Validation error: 0.46356034430471593\n",
            "Iteration: 2800 Training error: 0.4608123028899278 Validation error: 0.46389092553639755\n",
            "Iteration: 3200 Training error: 0.4605025905877822 Validation error: 0.4635498208315805\n",
            "Iteration: 3600 Training error: 0.4605626232110688 Validation error: 0.4636111745173458\n",
            "Iteration: 4000 Training error: 0.46076292854283973 Validation error: 0.46382505838656063\n",
            "Iteration: 4400 Training error: 0.46090748248681274 Validation error: 0.4639931631539435\n",
            "Iteration: 4800 Training error: 0.46075740721266 Validation error: 0.4638182865127096\n",
            "Iteration: 5200 Training error: 0.4605266576073126 Validation error: 0.4635730180028106\n",
            "Iteration: 5600 Training error: 0.46053016224297405 Validation error: 0.4635766960169302\n",
            "Iteration: 6000 Training error: 0.4605269641458348 Validation error: 0.463572525599786\n",
            "Iteration: 6400 Training error: 0.46080928948378197 Validation error: 0.46387268473789284\n",
            "Iteration: 6800 Training error: 0.4605273542724433 Validation error: 0.4635737350159449\n",
            "Iteration: 7200 Training error: 0.4606872978324912 Validation error: 0.46374561316000185\n",
            "Iteration: 7600 Training error: 0.4607932072931262 Validation error: 0.4638556965948064\n",
            "Iteration: 8000 Training error: 0.4605318072915653 Validation error: 0.46357840314183535\n",
            "Iteration: 8400 Training error: 0.46082886820023683 Validation error: 0.4638933524232731\n",
            "Iteration: 8800 Training error: 0.46081554955743165 Validation error: 0.46395217696471414\n",
            "Iteration: 9200 Training error: 0.4605233506604234 Validation error: 0.46356943426793173\n",
            "Iteration: 9600 Training error: 0.46053189858213334 Validation error: 0.46357850008794654\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.119617558463787 Validation error: 7.173932345072785\n",
            "Iteration: 400 Training error: 3.2019861094262145 Validation error: 3.2527124602526527\n",
            "Iteration: 800 Training error: 0.46848327515691135 Validation error: 0.4475237611972501\n",
            "Iteration: 1200 Training error: 0.4637017492999109 Validation error: 0.4380415521133795\n",
            "Iteration: 1600 Training error: 0.4635522513909336 Validation error: 0.4379180902146883\n",
            "Iteration: 2000 Training error: 0.46866969766000843 Validation error: 0.44206159245930005\n",
            "Iteration: 2400 Training error: 0.6619413355726289 Validation error: 0.7056950587651105\n",
            "Iteration: 2800 Training error: 0.5085158093807036 Validation error: 0.5030295927160915\n",
            "Iteration: 3200 Training error: 0.4636439642128912 Validation error: 0.4391573690138797\n",
            "Iteration: 3600 Training error: 0.4637762761611649 Validation error: 0.4382095747115933\n",
            "Iteration: 4000 Training error: 0.4674747874892069 Validation error: 0.4434998538515393\n",
            "Iteration: 4400 Training error: 0.46370910164405815 Validation error: 0.438213044885915\n",
            "Iteration: 4800 Training error: 0.46356169620239374 Validation error: 0.43801283755766107\n",
            "Iteration: 5200 Training error: 0.47813292791958784 Validation error: 0.45135250700066976\n",
            "Iteration: 5600 Training error: 0.46340241098172363 Validation error: 0.4376453895046405\n",
            "Iteration: 6000 Training error: 0.4634872570610161 Validation error: 0.43847883326387227\n",
            "Iteration: 6400 Training error: 0.4722940995554752 Validation error: 0.4486530166184768\n",
            "Iteration: 6800 Training error: 0.4637185659636763 Validation error: 0.43834361803694405\n",
            "Iteration: 7200 Training error: 0.4683994647460997 Validation error: 0.44262288032479824\n",
            "Iteration: 7600 Training error: 0.4638252611695225 Validation error: 0.4381757295827288\n",
            "Iteration: 8000 Training error: 0.49363170233056874 Validation error: 0.4693169175718186\n",
            "Iteration: 8400 Training error: 0.46362464288355576 Validation error: 0.4379463752389875\n",
            "Iteration: 8800 Training error: 0.46352658019414983 Validation error: 0.4382058206037321\n",
            "Iteration: 9200 Training error: 0.463494194474529 Validation error: 0.43778317930671257\n",
            "Iteration: 9600 Training error: 0.4635132756297339 Validation error: 0.4380946203756193\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Iteration: 0 Training error: 7.118728641032067 Validation error: 7.181937389664357\n",
            "Iteration: 400 Training error: 3.204042365731434 Validation error: 3.252994989158424\n",
            "Iteration: 800 Training error: 0.46693959592926515 Validation error: 0.45014391502566586\n",
            "Iteration: 1200 Training error: 0.5141074654966649 Validation error: 0.5089228793593834\n",
            "Iteration: 1600 Training error: 0.46273236154604314 Validation error: 0.44566416213633664\n",
            "Iteration: 2000 Training error: 0.4626901513928922 Validation error: 0.44577225997291536\n",
            "Iteration: 2400 Training error: 0.46378842500223266 Validation error: 0.44677771735183003\n",
            "Iteration: 2800 Training error: 0.46256688382076844 Validation error: 0.4455687184121047\n",
            "Iteration: 3200 Training error: 0.46281724432081917 Validation error: 0.44581982812962834\n",
            "Iteration: 3600 Training error: 0.46353912971196665 Validation error: 0.4464324533368876\n",
            "Iteration: 4000 Training error: 0.46520727900301373 Validation error: 0.4488792367830359\n",
            "Iteration: 4400 Training error: 0.46326061095605575 Validation error: 0.4465785603945424\n",
            "Iteration: 4800 Training error: 0.46270265302841757 Validation error: 0.4457153192587381\n",
            "Iteration: 5200 Training error: 0.4657420785039445 Validation error: 0.4477234040600107\n",
            "Iteration: 5600 Training error: 0.46259666421845114 Validation error: 0.4455776335921301\n",
            "Iteration: 6000 Training error: 0.46252372557399823 Validation error: 0.44555139509218095\n",
            "Iteration: 6400 Training error: 0.46347411700616264 Validation error: 0.4468542679124067\n",
            "Iteration: 6800 Training error: 2.765792763479258 Validation error: 2.908772113203967\n",
            "Iteration: 7200 Training error: 1.2384788285983006 Validation error: 1.3310643304213583\n",
            "Iteration: 7600 Training error: 0.9795469066797924 Validation error: 1.0390299167258874\n",
            "Iteration: 8000 Training error: 0.7452110254051532 Validation error: 0.774295206711265\n",
            "Iteration: 8400 Training error: 0.5561391077287756 Validation error: 0.559409175926709\n",
            "Iteration: 8800 Training error: 0.4744619233536533 Validation error: 0.45735896784156177\n",
            "Iteration: 9200 Training error: 0.46281401602202626 Validation error: 0.4458125267393396\n",
            "Iteration: 9600 Training error: 0.46812371304972644 Validation error: 0.4532807791026707\n",
            "Thetas: [7.12511955e+00 5.16508412e-01 1.39470969e-04]\n",
            "Training Loss: 0.45795500732602806\n",
            "Validation Loss: 0.48980166303840095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU4kz7QRS5AO",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W66BTEUZGh4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLogisticRegression():\n",
        "\t\"\"\"\n",
        "\tMy implementation of Logistic Regression.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef sigmoid(self,z):\n",
        "\t\t\"\"\"\n",
        "\t\tFind the sigmoid value of z\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tz : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tvalue of z in sigmoid function\n",
        "\t\t\"\"\"\n",
        "\t\treturn 1/(1+np.exp(-z))\n",
        "\tdef accuracy(self,y_hat,y):\n",
        "\t\tm=y.shape[0]\n",
        "\t\ty_hat[y_hat>=0.5]=1\n",
        "\t\ty_hat[y_hat<0.5]=0\n",
        "\t\treturn (1-sum(abs(y-y_hat))/m)*100\n",
        "\n",
        "\tdef cost_diff(self,X,y,theta):\n",
        "\t\t\"\"\"\n",
        "\t\tFind Log Loss error in current model parameters\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\ttheta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tderv : derivative of cost at the value theta\n",
        "\t\t\"\"\"\n",
        "\t\tm=y.shape[0]\n",
        "\t\tX_trans=X.T\t\t  \t# Transpose of vector X\n",
        "\t\tz=X.dot(theta)\n",
        "\t\tactiv =self.sigmoid(z)\n",
        "\n",
        "\t\tderv =(X_trans.dot(activ-y))\t\t  # Calculates X` * ( sigmoid(X*theta) - y )\n",
        "\n",
        "\t\terr=(-1/m)*(np.sum(y*np.log(activ)+(1-y)*np.log(1-activ)))\n",
        "\t\n",
        "\t\taccuracy=self.accuracy(activ,y)\n",
        "\n",
        "\t\treturn derv,err,accuracy\n",
        "\n",
        "\n",
        "\tdef stochastic_gradient_descent(self,X,y,X_test=None,y_test=None,epoch=100,alpha=0.01):\n",
        "\t\t\"\"\"\n",
        "\t\tFinding theta using the stochastic gradient descent model\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tepochs : Number of times gradient descent has to run\n",
        "\n",
        "\t\talpha : Learning rate of gradient descent\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ttheta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\t\t\"\"\"\n",
        "\t\tm=y.shape[0]\n",
        "\t\ttheta= np.zeros((X.shape[1],))\t\t  # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "\t\t\n",
        "\t\tvalidation_loss_list=np.array([])\n",
        "\t\ttraining_loss_list=np.array([])\n",
        "\t\tvalidation_acc_list=np.array([])\n",
        "\t\ttraining_acc_list=np.array([])\n",
        "\t\tfor i in range(m):\n",
        "\t\t\tcurr_X=np.array([X[i]])\n",
        "\t\t\tcurr_y=np.array([y[i]])\n",
        "\n",
        "\t\t\ttheta,val_loss,train_loss,val_acc,train_acc=self.batch_gradient_descent(curr_X,curr_y,X_test,y_test,theta)\n",
        "\t \n",
        "\t\t\tvalidation_loss_list=np.concatenate((validation_loss_list,val_loss))\n",
        "\t\t\ttraining_loss_list=np.concatenate((training_loss_list,train_loss))\n",
        "\t\t\ttraining_acc_list=np.append(training_acc_list,train_acc)\n",
        "\t\t\tvalidation_acc_list=np.append(validation_acc_list,val_acc)\n",
        "\t\n",
        "\t\t\n",
        "\t\treturn theta,validation_loss_list,training_loss_list,validation_acc_list,training_acc_list\n",
        "\n",
        "\t\n",
        "\tdef batch_gradient_descent(self,X,y,X_test=None,y_test=None,theta=None,epoch=1000,alpha=0.01):\n",
        "\t\t\"\"\"\n",
        "\t\tFinding theta using the batch gradient descent model\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tepochs : Number of times gradient descent has to run\n",
        "\n",
        "\t\talpha : Learning rate of gradient descent\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ttheta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\t\t\"\"\"\n",
        "\t\tm=y.shape[0]\n",
        "\t\tif(theta is None):\n",
        "\t\t\ttheta= np.zeros((X.shape[1],))\t\t  # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "\t\t\n",
        "\t\tvalidation_loss_list=np.array([])\n",
        "\t\ttraining_loss_list=np.array([])\n",
        "\t\n",
        "\t\tvalidation_acc_list=np.array([])\n",
        "\t\ttraining_acc_list=np.array([])\n",
        "\n",
        "\t\tfor i in range(epoch):\n",
        "\t\t\tderv_train,loss_train,accuracy_train=self.cost_diff(X,y,theta)\n",
        "\t\t\ttraining_loss_list=np.append(training_loss_list,loss_train)\n",
        "\t\t\ttraining_acc_list=np.append(training_acc_list,accuracy_train)\n",
        "\n",
        "\t\t\tif(X_test is not None):\n",
        "\t\t\t\tderv_val,loss_val,accuracy_val=self.cost_diff(X_test,y_test,theta)\n",
        "\t\t\t\tvalidation_loss_list=np.append(validation_loss_list,loss_val)\n",
        "\t\t\t\tvalidation_acc_list=np.append(validation_acc_list,accuracy_val)\n",
        "\t\t\n",
        "\t\t\ttheta=theta-(alpha/m)*derv_train\n",
        "\t\t\n",
        "\t\treturn theta,validation_loss_list,training_loss_list,validation_acc_list,training_acc_list\n",
        "\n",
        "\tdef fit(self, X, y,X_test=None,y_test=None,epoch=10000,alpha=0.01):\n",
        "\t\t\"\"\"\n",
        "\t\tFitting (training) the logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tself : an instance of self\n",
        "\t\t\"\"\"\n",
        "\t\tX=np.concatenate((np.ones((X.shape[0],1)),X),axis=1) # Adding a bias variable i.e columns of 1 to data\n",
        "\n",
        "\t\tif(X_test is not None): # if validation set is provided then add a bias variable i.e columns of 1 to data\n",
        "\t\t\tX_test=np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis=1)\n",
        "\t\t\n",
        "\t\t# self.theta,self.validation_loss,self.training_loss,self.validation_acc,self.training_acc = self.batch_gradient_descent(X,y,X_test,y_test,epoch=epoch,alpha=alpha)\t\t  # using the gradient descent method with given number of epochs and learning rate\n",
        "\t\t\n",
        "\t\tself.theta,self.validation_loss,self.training_loss,self.validation_acc,self.training_acc = self.stochastic_gradient_descent(X,y,X_test,y_test,epoch=10,alpha=alpha)\n",
        "\t\t\n",
        "\t\t# fit function has to return an instance of itself or else it won't work with test.py\n",
        "\t\treturn self\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\t\"\"\"\n",
        "\t\tPredicting values using the trained logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "\t\t\"\"\"\n",
        "\t\tX=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
        "\t\ty=self.sigmoid(X.dot(self.theta))\n",
        "\t\n",
        "\t\ty[y>=0.5]=1\n",
        "\t\ty[y<0.5]=0\n",
        "\n",
        "\t\t# return the numpy array y which contains the predicted values\n",
        "\t\treturn y\n",
        "\n",
        "\tdef plot_loss(self):\n",
        "\t\tprint(\"Thetas:\",self.theta)\n",
        "\t\tprint(\"Training Loss:\",self.training_loss[-1])\n",
        "\t\tprint(\"Validation Loss:\",self.validation_loss[-1])\n",
        "\n",
        "\t\tx=np.arange(self.training_loss.shape[0])\n",
        "\t\tplt.plot(x,self.training_loss,color=\"g\", label=\"Training Loss\")\n",
        "\t\tplt.plot(x,self.validation_loss,color=\"r\",label=\"Validation Loss\")\n",
        "\t\tplt.xlabel('Iteration')\n",
        "\t\tplt.ylabel('Loss')\n",
        "\t\tplt.legend()\n",
        "\t\tplt.show()\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erIgoN4nvyyr",
        "colab_type": "text"
      },
      "source": [
        "## Dataset 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYa7lf2fv11j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "a3823fed-8003-4106-e885-52706b41b17e"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(2)\n",
        "X_test=X[:X.shape[0]//8]\n",
        "y_test=y[:y.shape[0]//8]\n",
        "\n",
        "X_train=X[X.shape[0]//8:]\n",
        "y_train=y[y.shape[0]//8:]\n",
        "\n",
        "logistic = MyLogisticRegression()\n",
        "logistic.fit(X_train, y_train,X_test,y_test,alpha=0.01)\n",
        "print(logistic.training_acc[-1],logistic.validation_acc[-1])\n",
        "logistic.plot_loss()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.0 97.6608187134503\n",
            "Thetas: [ -3.5622346  -11.7270474  -11.95750797 -13.41139003  -0.90147196]\n",
            "Training Loss: 4.224444010946235e-07\n",
            "Validation Loss: 0.0625727794641759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa9AxYh_cOMD",
        "colab_type": "text"
      },
      "source": [
        "## Sklearn Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oDRuzR-cRRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQhMTMPxcTy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f2e39074-ce84-4321-f8c6-3b3de278783a"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "x, y = preprocessor.pre_process(2)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=4)\n",
        "logistic_regression = LogisticRegression()\n",
        "logistic_regression.fit(x_train,y_train)\n",
        "y_pred = logistic_regression.predict(x_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Test:\",accuracy)\n",
        "y_pred1 = logistic_regression.predict(x_train)\n",
        "accuracy = metrics.accuracy_score(y_train, y_pred1)\n",
        "print(\"Accuracy Train:\",accuracy)\n",
        "print(\"Thetas:\",logistic_regression.coef_)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Test: 0.9795918367346939\n",
            "Accuracy Train: 0.9825072886297376\n",
            "Thetas: [[-4.58128443 -4.68888037 -4.27457391  0.23536925]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOUVjO6MSZBi",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdbZwPiFGwiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3f8b307d-02e5-4467-8623-aa233a182bba"
      },
      "source": [
        "Xtrain = np.array([[1, 2, 3], \n",
        "                   [4, 5, 6]])\n",
        "ytrain = np.array([1, 2])\n",
        "\n",
        "Xtest = np.array([[7, 8, 9]])\n",
        "ytest = np.array([3])\n",
        "\n",
        "print('Linear Regression')\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(Xtrain, ytrain)\n",
        "\n",
        "ypred = linear.predict(Xtest)\n",
        "\n",
        "print('Predicted Values:', ypred)\n",
        "print('True Values:', ytest)\n",
        "\n",
        "print('Logistic Regression')\n",
        "\n",
        "logistic = MyLogisticRegression()\n",
        "logistic.fit(Xtrain, ytrain)\n",
        "\n",
        "ypred = logistic.predict(Xtest)\n",
        "\n",
        "print('Predicted Values:', ypred)\n",
        "print('True Values:', ytest)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Predicted Values: [3.47084775]\n",
            "True Values: [3]\n",
            "Logistic Regression\n",
            "Predicted Values: [1.]\n",
            "True Values: [3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}