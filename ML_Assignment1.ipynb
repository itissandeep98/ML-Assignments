{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfdqD6RYDNJwr5DcyvyV1l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/ML_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEo5gx2hZQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSTeIMNDSrEl",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofY4sQaGeYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLinearRegression():\n",
        "  \"\"\"\n",
        "\tMy implementation of Linear Regression.\n",
        "\t\"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def cross_validation(self,data,epoch=100,alpha=0.01,k=10):\n",
        "    m=data.shape[0]\n",
        "    split_start=0\n",
        "    split_end=m//k\n",
        "    theta_list=[0]*k\n",
        "    training_loss_list=[0]*k\n",
        "    validation_loss_list=[0]*k\n",
        "    error_min=float(\"inf\")\n",
        "    idx=0\n",
        "    for i in range(k):\n",
        "\n",
        "      test=data[split_start:split_end,:]\n",
        "      train=np.concatenate((data[:split_start,:],data[split_end:,:]),axis=0)\n",
        "\n",
        "      split_start=split_end\n",
        "      split_end+=m//k\n",
        "\n",
        "      self.fit(train[:,1:],train[:,0],epoch,alpha)\n",
        "      prediction=self.predict(test[:,1:])\n",
        "\n",
        "      theta_list[i]=self.theta\n",
        "      training_loss_list[i]=self.training_loss\n",
        "      validation_loss_list[i]=abs(prediction-test[:,0])\n",
        "\n",
        "      error=sum(validation_loss_list[i])\n",
        "\n",
        "      if(error<error_min):\n",
        "        idx=i\n",
        "        error_min=error\n",
        "        \n",
        "    self.theta=theta_list[idx]\n",
        "    self.training_loss=training_loss_list[idx]\n",
        "    self.validation_loss=validation_loss_list[idx]\n",
        "    \n",
        "    \n",
        "      \n",
        "\n",
        "  def gradient_descent(self,X,y,epochs=200,alpha=0.01):\n",
        "    \"\"\"\n",
        "    Finding theta using the gradient descent model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    epochs : Number of times gradient descent has to run\n",
        "\n",
        "    alpha : Learning rate of gradient descent\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "    theta=np.transpose(np.array([0]*len(X[0])))                      # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "    training_loss=np.array([])\n",
        "    for i in range(epochs):\n",
        "      theta=theta-(alpha/m)*self.RMSE(X,y,theta)\n",
        "      training_loss=np.append(training_loss,sum(abs(X.dot(theta)-y)))\n",
        "   \n",
        "    return (theta,training_loss)\n",
        "\n",
        "  def RMSE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Root Mean Squared Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    \"\"\"\n",
        "    X_trans=np.transpose(X)                                           # Transpose of vector X\n",
        "    derv =(X_trans.dot(X.dot(theta)-y))                               # Calculates X` * ( X*theta - y )\n",
        "\n",
        "    return derv\n",
        "\n",
        "  def fit(self, X, y,epoch=400,alpha=0.01):\n",
        "    \"\"\"\n",
        "    Fitting (training) the linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self : an instance of self\n",
        "    \"\"\"\n",
        "    X_trans=np.transpose(X)\n",
        "    # try:\n",
        "    #   self.theta = np.linalg.inv(X_trans.dot(X)).dot(X_trans).dot(y)  # using the normal eqn, theta = inv(X`*X)*X`*y\n",
        "    # except:\n",
        "    self.theta,self.training_loss = self.gradient_descent(X,y,epoch,alpha) # using the gradient descent method with given number of epochs and learning rate\n",
        "\n",
        "    # fit function has to return an instance of itself or else it won't work with test.py\n",
        "    return self\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predicting values using the trained linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "    \"\"\"\n",
        "    \n",
        "    y=np.dot(X,self.theta)\n",
        "    # return the numpy array y which contains the predicted values\n",
        "    return y\n",
        "\n",
        "  def plot_loss(self,y):\n",
        "    x=np.arange(y.shape[0])\n",
        "    plt.plot(x,y,color=\"g\")\n",
        "    plt.show()\n",
        "    \n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHOIS1Wb9tpj"
      },
      "source": [
        "## Dataset2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpSzdHFk9tpz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "aff0463c-ba00-44ad-d24c-05c9de3c374c"
      },
      "source": [
        "x=pd.read_csv('/content/VideoGameDataset.csv')\n",
        "x.dropna(inplace=True)\n",
        "# print(x[['User_Score','Critic_Score','Global_Sales']])\n",
        "\n",
        "data=x[['User_Score','Critic_Score','Global_Sales']]\n",
        "data=data.to_numpy().astype(np.float)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.cross_validation(data,400,0.0001)\n",
        "\n",
        "print(linear.theta)\n",
        "\n",
        "linear.plot_loss(linear.training_loss)\n",
        "\n",
        "# linear.plot_loss(linear.validation_loss)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.10125076 -0.01192254]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68z99ULqRBt",
        "colab_type": "text"
      },
      "source": [
        "## Dataset1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrZ20anXvGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=pd.read_csv('/content/Dataset.data',delim_whitespace=True,header=None)\n",
        "x[0].replace('M',1,inplace=True)\n",
        "x[0].replace('F',2,inplace=True)\n",
        "x[0].replace('I',3,inplace=True)\n",
        "x=x.to_numpy()\n",
        "a=np.transpose([x[:,-1]])\n",
        "b=x[:,:-1]\n",
        "data=np.concatenate((a,b),axis=1)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.cross_validation(data)\n",
        "print(linear.theta)\n",
        "print(linear.training_loss.shape,x.shape)\n",
        "print(linear.validation_loss.shape)\n",
        "linear.plot_loss(linear.training_loss)\n",
        "linear.plot_loss(linear.validation_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU4kz7QRS5AO",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W66BTEUZGh4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLogisticRegression():\n",
        "\t\"\"\"\n",
        "\tMy implementation of Logistic Regression.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef sigmoid(self,z):\n",
        "\t\t\"\"\"\n",
        "\t\tFind the sigmoid value of z\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tz : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tvalue of z in sigmoid function\n",
        "\t\t\"\"\"\n",
        "\t\treturn 1/(1+np.exp(-z))\n",
        "\t\n",
        "\tdef cost_diff(self,X,y,theta):\n",
        "\t\t\"\"\"\n",
        "\t\tFind Log Loss error in current model parameters\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\ttheta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tderv : derivative of cost at the value theta\n",
        "\t\t\"\"\"\n",
        "\t\tX_trans=np.transpose(X)\t\t\t\t\t\t\t\t\t\t                       # Transpose of vector X\n",
        "\t\t\n",
        "\t\tderv =(X_trans.dot(self.sigmoid(X.dot(theta))-y))\t\t\t\t         # Calculates X` * ( sigmoid(X*theta) - y )\n",
        "\t\treturn derv\n",
        "\n",
        "\t\n",
        "\tdef gradient_descent(self,X,y,epochs=100,alpha=0.01):\n",
        "\t\t\"\"\"\n",
        "\t\tFinding theta using the gradient descent model\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tepochs : Number of times gradient descent has to run\n",
        "\n",
        "\t\talpha : Learning rate of gradient descent\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ttheta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\t\t\"\"\"\n",
        "\t\tm=len(y)\n",
        "\t\ttheta=np.transpose(np.array([0]*len(X[0])))                     # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "\t\t\n",
        "\t\tfor i in range(epochs):\n",
        "\t\t  theta=theta-(alpha/m)*self.cost_diff(X,y,theta)\n",
        "\t\t\n",
        "\t\treturn theta\n",
        "\n",
        "\tdef fit(self, X, y):\n",
        "\t\t\"\"\"\n",
        "\t\tFitting (training) the logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tself : an instance of self\n",
        "\t\t\"\"\"\n",
        "\t\tself.theta = self.gradient_descent(X,y,200,0.01)              # using the gradient descent method with given number of epochs and learning rate\n",
        "\n",
        "\t\t# fit function has to return an instance of itself or else it won't work with test.py\n",
        "\t\treturn self\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\t\"\"\"\n",
        "\t\tPredicting values using the trained logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "\t\t\"\"\"\n",
        "\t\ty=self.sigmoid(X.dot(self.theta))\n",
        "\t\n",
        "\t\t# return the numpy array y which contains the predicted values\n",
        "\t\treturn y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOUVjO6MSZBi",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdbZwPiFGwiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "e47be93b-97ab-4f23-d56c-357cd25498db"
      },
      "source": [
        "Xtrain = np.array([[1, 2, 3], \n",
        "                   [4, 5, 6]])\n",
        "ytrain = np.array([1, 2])\n",
        "\n",
        "Xtest = np.array([[7, 8, 9]])\n",
        "ytest = np.array([3])\n",
        "\n",
        "print('Linear Regression')\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(Xtrain, ytrain)\n",
        "print(linear.theta)\n",
        "ypred = linear.predict(Xtest)\n",
        "\n",
        "print('Predicted Values:', ypred)\n",
        "print('True Values:', ytest)\n",
        "\n",
        "print('Logistic Regression')\n",
        "\n",
        "logistic = MyLogisticRegression()\n",
        "logistic.fit(Xtrain, ytrain)\n",
        "\n",
        "ypred = logistic.predict(Xtest)\n",
        "\n",
        "print('Predicted Values:', ypred)\n",
        "print('True Values:', ytest)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression\n",
            "[0.03028946 0.12308121 0.21587297]\n",
            "Predicted Values: [3.13953261]\n",
            "True Values: [3]\n",
            "Logistic Regression\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4532add385c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logistic Regression'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MyLogisticRegression' is not defined"
          ]
        }
      ]
    }
  ]
}