{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5cB/yJhelYyoIKeZ6dm1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/ML_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5O3iEtAFqzd",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEo5gx2hZQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9RJnOQAEAU3",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kS_U4I6EDez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyPreProcessor():\n",
        "    \"\"\"\n",
        "    My steps for pre-processing for the three datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def pre_process(self, dataset):\n",
        "        \"\"\"\n",
        "        Reading the file and preprocessing the input and output.\n",
        "        Note that you will encode any string value and/or remove empty entries in this function only.\n",
        "        Further any pre processing steps have to be performed in this function too. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        dataset : integer with acceptable values 0, 1, or 2\n",
        "        0 -> Abalone Dataset\n",
        "        1 -> VideoGame Dataset\n",
        "        2 -> BankNote Authentication Dataset\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "        y : 1-dimensional numpy array of shape (n_samples,)\n",
        "        \"\"\"     \n",
        "\n",
        "        if dataset == 0:\n",
        "            df=pd.read_csv('/content/Dataset.data',delim_whitespace=True,header=None) # data read from file\n",
        "            df.sample(frac=1) # data shuffled\n",
        "\n",
        "            # changed gender values to integers\n",
        "            df[0].replace('M',1,inplace=True)\n",
        "            df[0].replace('F',2,inplace=True)\n",
        "            df[0].replace('I',3,inplace=True)\n",
        "\n",
        "            data=df.to_numpy() # converted dataframe into numpy array\n",
        "            X=data[:,:-1]\n",
        "            y=data[:,-1]\n",
        "\n",
        "        elif dataset == 1:\n",
        "            df=pd.read_csv('/content/VideoGameDataset.csv') # data read from file\n",
        "            df=df[['Critic_Score','Global_Sales','User_Score']] # required colums extracted\n",
        "            df=df.sample(frac=1) # data shuffled\n",
        "\n",
        "            df['Critic_Score'].fillna(df['Critic_Score'].median(), inplace=True) # replaced NaN values with median of the column\n",
        "            df['User_Score'].replace(to_replace = 'tbd', value = np.nan,inplace=True) # replaced the cell with 'tbd' value to NaN value in the colum\n",
        "            df['User_Score']=df['User_Score'].astype(np.float) # converted column from strings to float values\n",
        "            df['User_Score'].fillna(df['User_Score'].median(), inplace=True) # replaced NaN values with median of the column\n",
        "\n",
        "            data=df.to_numpy() # converted dataframe into numpy array\n",
        "            X=data[:,:-1]\n",
        "            y=data[:,-1]\n",
        "\n",
        "        elif dataset == 2:\n",
        "            # Implement for the banknote authentication dataset\n",
        "            pass\n",
        "\n",
        "        X=(X-X.mean(axis=0))/X.std(axis=0) # normalized the data \n",
        "\n",
        "        return X, y\n"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSTeIMNDSrEl",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gofY4sQaGeYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLinearRegression():\n",
        "  \"\"\"\n",
        "\tMy implementation of Linear Regression.\n",
        "\t\"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def cross_validation(self,X,y,epoch=1000,alpha=0.01,k=10,lossfunc=1):\n",
        "    \"\"\"\n",
        "    performs k fold cross validation on the given dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) \n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    k : Number of folds the data needs to be splitted into\n",
        "\n",
        "    epoch : Number of times gradient descent has to run\n",
        "\n",
        "    alpha : Learning rate of gradient descent\n",
        "\n",
        "    lossfunc: determines which loss function to call\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=X.shape[0] # number of samples\n",
        "\n",
        "    split_start=0 # initial split's first index\n",
        "    split_end=m//k # initial split's last index\n",
        "\n",
        "    theta_list=[0]*k  # initialized theta\n",
        "    training_loss_list=[0]*k  # initialized list to store all the training loss from every fold\n",
        "    validation_loss_list=[0]*k # initialized list to store all the validation loss from every fold\n",
        "\n",
        "    error_min=float(\"inf\")\n",
        "    idx=0\n",
        "\n",
        "    for i in range(k):\n",
        "\n",
        "      # Extracting X and y for train and test set\n",
        "      X_train=np.concatenate((X[:split_start],X[split_end:]),axis=0)\n",
        "      y_train=np.concatenate((y[:split_start],y[split_end:]),axis=0)\n",
        "\n",
        "      X_test=X[split_start:split_end]\n",
        "      y_test=y[split_start:split_end]\n",
        "\n",
        "      self.fit(X_train,y_train,X_test,y_test,epoch,alpha,lossfunc) # calculating model parameters by running the gradient descent\n",
        "\n",
        "      # storing the results of current fold in the array\n",
        "      theta_list[i]=self.theta\n",
        "      training_loss_list[i]=self.training_loss\n",
        "      validation_loss_list[i]=self.validation_loss\n",
        "\n",
        "      split_start=split_end # updating slice parameters\n",
        "      split_end+=m//k\n",
        "\n",
        "      error=training_loss_list[i][-1]\n",
        "\n",
        "\n",
        "      # if the error in this fold is minimum of all errors seen upto now then update it and store the fold number\n",
        "      if(error<error_min):\n",
        "        idx=i\n",
        "        error_min=error\n",
        "    \n",
        "    # final storing the values associated with minimum error \n",
        "    self.theta=theta_list[idx]\n",
        "    self.training_loss=training_loss_list[idx]\n",
        "    self.validation_loss=validation_loss_list[idx]\n",
        "\n",
        "  def MSE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Mean Squared Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "\n",
        "    X_trans=np.transpose(X)                                           # Transpose of vector X\n",
        "    err=X.dot(theta)-y\n",
        "    derv =(1/m)*(X_trans.dot(err))                         # Calculates X` * ( X*theta - y )\n",
        "    \n",
        "    return derv,sum(err**2)/m    \n",
        " \n",
        "  def MAE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Mean Average Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    m=len(y)\n",
        "    err=(1/m)*(X.dot(theta)-y)\n",
        "    X_trans=X.T\n",
        "   \n",
        "    grad=(1/m)*(X_trans.dot(abs(err)/err))\n",
        "    \n",
        "    return grad,sum(abs(err))\n",
        "\n",
        "  def RMSE(self,X,y,theta):\n",
        "    \"\"\"\n",
        "    finding Root Mean Squared Error based on current model parameters\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) \n",
        "\n",
        "    theta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    derv : derivative of cost at the value theta\n",
        "    err  : Error/cost in prediction at the value theta\n",
        "    \"\"\"\n",
        "    X_trans=X.T                                               # Transpose of vector X\n",
        "    y_hat=X.dot(theta)-y\n",
        "    m=len(y)\n",
        "   \n",
        "    err=((1/m)*np.sum((y_hat)**2))**0.5\n",
        "    derv =(1/m)*(X_trans.dot(y_hat))/err         \n",
        "    \n",
        "    return derv,err\n",
        "\n",
        "\n",
        "  def gradient_descent(self,X,y,X_test,y_test,epochs,alpha,lossfunc):\n",
        "    \"\"\"\n",
        "    Finding theta using the gradient descent method\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    X_test : 2-dimensional numpy array of shape (n_samples, n_features) which acts as Testing data.\n",
        "\n",
        "    y_test : 1-dimensional numpy array of shape (n_samples,) which acts as Testing labels.\n",
        "\n",
        "    epochs : Number of times gradient descent has to run\n",
        "\n",
        "    alpha : Learning rate of gradient descent\n",
        "\n",
        "    lossfunc: determines which loss function to call\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    theta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\n",
        "    training_loss: Calculated training loss at every theta\n",
        "\n",
        "    tvalidation_loss: Calculated validation loss at every theta\n",
        "    \"\"\"\n",
        "   \n",
        "    theta= np.zeros((X.shape[1],))                      # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "    \n",
        "    training_loss=np.array([])  # initializing array to store training loss at every value of theta\n",
        "    validation_loss= np.array([]) # initializing array to store validation loss at every value of theta\n",
        "\n",
        "    # print(\"-*\"*20)\n",
        "\n",
        "    for i in range(epochs): \n",
        "      if(lossfunc==1): \n",
        "        derv,train_loss=self.RMSE(X,y,theta)   \n",
        "      elif(lossfunc==2):   \n",
        "        derv,train_loss=self.MAE(X,y,theta)\n",
        "      else:\n",
        "        derv,train_loss=self.MSE(X,y,theta)\n",
        "      training_loss=np.append(training_loss,train_loss)\n",
        "\n",
        "      # if(i%400==0):\n",
        "      #   print(\"Iteration:\",i,\"Training error:\",train_loss)\n",
        "\n",
        "      if(X_test is not None): # calculate validation loss only if test set is provided\n",
        "        if(lossfunc==1):\n",
        "          derv_val,val_loss=self.RMSE(X_test,y_test,theta)   \n",
        "        elif(lossfunc==2):   \n",
        "          derv_val,val_loss=self.MAE(X_test,y_test,theta)\n",
        "        else:\n",
        "          derv_val,val_loss=self.MSE(X_test,y_test,theta)\n",
        "      \n",
        "        validation_loss=np.append(validation_loss,val_loss)\n",
        "        \n",
        "      theta=theta-alpha*derv\n",
        "      \n",
        "  \n",
        "    return theta,training_loss,validation_loss\n",
        "\n",
        "  def fit(self, X, y,X_test=None,y_test=None,epoch=400,alpha=0.01,lossfunc=1):\n",
        "    \"\"\"\n",
        "    Fitting (training) the linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    self : an instance of self\n",
        "    \"\"\"\n",
        "\n",
        "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1) # Adding a bias variable i.e columns of 1 to data\n",
        "\n",
        "    if(X_test is not None): # if validation set is provided then add a bias variable i.e columns of 1 to data\n",
        "      X_test=np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis=1)\n",
        "   \n",
        "    X_trans=np.transpose(X)\n",
        "    if(lossfunc==4):\n",
        "      try:\n",
        "        self.theta = np.linalg.inv(X_trans.dot(X)).dot(X_trans).dot(y)  # using the normal eqn, theta = inv(X`*X)*X`*y\n",
        "      except:\n",
        "        self.theta,self.training_loss,self.validation_loss = self.gradient_descent(X,y,X_test,y_test,epoch,alpha,lossfunc=1) # using the gradient descent method with RMSE loss function if the given data is non invertible\n",
        "    else: \n",
        "      self.theta,self.training_loss,self.validation_loss = self.gradient_descent(X,y,X_test,y_test,epoch,alpha,lossfunc) # using the gradient descent method with given number of epochs and learning rate\n",
        "      \n",
        "\n",
        "    # fit function has to return an instance of itself or else it won't work with test.py\n",
        "    return self\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predicting values using the trained linear model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "    \"\"\"\n",
        "    X=np.concatenate((np.ones((X.shape[0],1)),X),axis=1)\n",
        "    y=np.dot(X,self.theta)\n",
        "    # return the numpy array y which contains the predicted values\n",
        "    return y\n",
        "\n",
        "  def plot_loss(self):\n",
        "    print(\"Thetas:\",self.theta)\n",
        "    print(\"Training Loss:\",self.training_loss[-1])\n",
        "    print(\"Validation Loss:\",self.validation_loss[-1])\n",
        "    \n",
        "    x=np.arange(self.training_loss.shape[0])\n",
        "    plt.plot(x,self.training_loss,color=\"g\", label=\"Training Loss\")\n",
        "    plt.plot(x,self.validation_loss,color=\"r\",label=\"Validation Loss\")\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E68z99ULqRBt",
        "colab_type": "text"
      },
      "source": [
        "## Dataset1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrZ20anXvGL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "1dabdfb1-4e17-4aa0-89b4-002eff50f249"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(0)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(X,y,lossfunc=4)\n",
        "print(linear.theta)\n",
        "linear.cross_validation(X,y,epoch=10000)\n",
        "linear.plot_loss()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 9.93368446 -0.3215536  -0.09923279  1.18716678  0.46859482  4.44739825\n",
            " -4.46228469 -1.11301053  1.21107218]\n",
            "Thetas: [ 9.73650399 -0.3482582   0.15797641  0.74317607  0.40411382  0.89044654\n",
            " -2.52999649 -0.31301375  2.37495373]\n",
            "Training Loss: 2.0486255251475742\n",
            "Validation Loss: 3.524723068996929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHOIS1Wb9tpj"
      },
      "source": [
        "## Dataset2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpSzdHFk9tpz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "f3cafedd-c017-448d-afff-4ef9d11a2213"
      },
      "source": [
        "preprocessor = MyPreProcessor()\n",
        "X, y = preprocessor.pre_process(1)\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(X,y,lossfunc=4)\n",
        "print(linear.theta)\n",
        "linear.cross_validation(X,y)\n",
        "linear.plot_loss()"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 7.32978049  0.5003368  -0.04484104]\n",
            "Thetas: [ 7.3322366   0.5002543  -0.04294719]\n",
            "Training Loss: 0.7956840347214154\n",
            "Validation Loss: 0.9623461074764498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU4kz7QRS5AO",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W66BTEUZGh4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLogisticRegression():\n",
        "\t\"\"\"\n",
        "\tMy implementation of Logistic Regression.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef sigmoid(self,z):\n",
        "\t\t\"\"\"\n",
        "\t\tFind the sigmoid value of z\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tz : 1-dimensional numpy array of shape (n_samples,)\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tvalue of z in sigmoid function\n",
        "\t\t\"\"\"\n",
        "\t\treturn 1/(1+np.exp(-z))\n",
        "\t\n",
        "\tdef cost_diff(self,X,y,theta):\n",
        "\t\t\"\"\"\n",
        "\t\tFind Log Loss error in current model parameters\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\ttheta : Value of theta at which derivative of cost has to be found\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tderv : derivative of cost at the value theta\n",
        "\t\t\"\"\"\n",
        "\t\tX_trans=np.transpose(X)\t\t\t\t\t\t\t\t\t\t                       # Transpose of vector X\n",
        "\t\t\n",
        "\t\tderv =(X_trans.dot(self.sigmoid(X.dot(theta))-y))\t\t\t\t         # Calculates X` * ( sigmoid(X*theta) - y )\n",
        "\t\treturn derv\n",
        "\n",
        "\t\n",
        "\tdef gradient_descent(self,X,y,epochs=100,alpha=0.01):\n",
        "\t\t\"\"\"\n",
        "\t\tFinding theta using the gradient descent model\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tepochs : Number of times gradient descent has to run\n",
        "\n",
        "\t\talpha : Learning rate of gradient descent\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ttheta : Calculated value of theta on given test set (X,y) with learning rate alpha \n",
        "\t\t\"\"\"\n",
        "\t\tm=len(y)\n",
        "\t\ttheta=np.transpose(np.array([0]*len(X[0])))                     # created a column vector theta of length equal to number of features in X with all the initial values 0\n",
        "\t\t\n",
        "\t\tfor i in range(epochs):\n",
        "\t\t  theta=theta-(alpha/m)*self.cost_diff(X,y,theta)\n",
        "\t\t\n",
        "\t\treturn theta\n",
        "\n",
        "\tdef fit(self, X, y):\n",
        "\t\t\"\"\"\n",
        "\t\tFitting (training) the logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
        "\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tself : an instance of self\n",
        "\t\t\"\"\"\n",
        "\t\tself.theta = self.gradient_descent(X,y,200,0.01)              # using the gradient descent method with given number of epochs and learning rate\n",
        "\n",
        "\t\t# fit function has to return an instance of itself or else it won't work with test.py\n",
        "\t\treturn self\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\t\"\"\"\n",
        "\t\tPredicting values using the trained logistic model.\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
        "\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\ty : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
        "\t\t\"\"\"\n",
        "\t\ty=self.sigmoid(X.dot(self.theta))\n",
        "\t\n",
        "\t\t# return the numpy array y which contains the predicted values\n",
        "\t\treturn y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOUVjO6MSZBi",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdbZwPiFGwiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5e04d0c6-4fad-455b-a72d-ef669eb12e57"
      },
      "source": [
        "Xtrain = np.array([[1, 2, 3], \n",
        "                   [4, 5, 6]])\n",
        "ytrain = np.array([1, 2])\n",
        "\n",
        "Xtest = np.array([[7, 8, 9]])\n",
        "ytest = np.array([3])\n",
        "\n",
        "print('Linear Regression')\n",
        "\n",
        "linear = MyLinearRegression()\n",
        "linear.fit(Xtrain, ytrain)\n",
        "\n",
        "ypred = linear.predict(Xtest)\n",
        "\n",
        "print('Predicted Values:', ypred)\n",
        "print('True Values:', ytest)\n",
        "\n",
        "# print('Logistic Regression')\n",
        "\n",
        "# logistic = MyLogisticRegression()\n",
        "# logistic.fit(Xtrain, ytrain)\n",
        "\n",
        "# ypred = logistic.predict(Xtest)\n",
        "\n",
        "# print('Predicted Values:', ypred)\n",
        "# print('True Values:', ytest)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression\n",
            "Predicted Values: [3.47084775]\n",
            "True Values: [3]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}